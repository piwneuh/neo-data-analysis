1686160529137:library(sparklyr)
1686160529642:library(dplyr)
1686160529660:# Install
1686160529660:spark_install()
1686160565673:# Connect
1686160565673:sc <- sparklyr::spark_connect(master = "local")
1686160569637:datasetPath <- "/data/dataset.csv"
1686160569637:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686160581401:glimpse(df)
1686160582170:head(df)
1686160582736:spark_disconnect(sc)
1686161627258:# Display column names
1686161627259:colnames(data)
1686161630303:# Display summary statistics
1686161630304:summary(data)
1686161634525:# Explore data distribution
1686161634527:hist(data$column_name)
1686161643488:# Connect
1686161643489:sc <- sparklyr::spark_connect(master = "local")
1686161647219:datasetPath <- "/data/dataset.csv"
1686161649549:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686161783417:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686161798877:# Display column names
1686161798879:colnames(df)
1686161802481:# Display summary statistics
1686161802482:summary(df)
1686161805179:# Explore data distribution
1686161805180:hist(df$column_name)
1686162217011:hist(df$diameter)
1686162323146:hist(diameter)
1686162501435:# Exploring data
1686162501436:colnames(df)
1686162503769:summary(df)
1686162507031:hist()
1686162530453:df$diameter
1686162534647:df$diameter
1686162673286:# Exploring data
1686162673287:colSums(df)
1686162698676:# Exploring data
1686162698677:colnames(df)
1686162702456:summary(df)
1686162905134:selected_df <- df %>%
1686162905135:select(id, full_name, H, diameter)
1686162925472:grouped_df <- df %>%
1686162925473:group_by(class) %>%
1686162925474:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686162942975:local_df <- collect(selected_df)
1686162960851:selected_df
1686162967083:grouped_df
1686162979728:local_df
1686163154975:# Classification
1686163154977:library(caret)
1686163172763:# Split the data into training and testing sets
1686163172763:set.seed(123)  # For reproducibility
1686163204158:train_indices <- createdfPartition(df$target_variable, p = 0.7, list = FALSE)
1686163208629:# Classification
1686163208630:library(caret)
1686163273513:# Classification
1686163273514:# Install and load required packages
1686163273514:install.packages("mlr")
1686163532224:library(mlr)
1686163532225:set.seed(123)
1686163532225:train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
1686163532244:training_set <- df[train_indices, ]
1686163532244:testing_set <- df[-train_indices, ]
1686163532245:train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
1686163532262:training_set <- df[train_indices, ]
1686163532262:testing_set <- df[-train_indices, ]
1686163532263:training_set
1686163532263:testing_set
1686163532263:trained_model1 <- train(model1, training_set)
1686163532264:trained_model2 <- train(model2, training_set)
1686163532264:trained_model3 <- train(model3, training_set)
1686163532264:trained_model4 <- train(model4, training_set)
1686163532265:trained_model5 <- train(model5, training_set)
1686163532265:# Define the classification models
1686163532265:model1 <- makeLearner("classif.logreg")
1686163532265:model2 <- makeLearner("classif.rpart")
1686163532266:model3 <- makeLearner("classif.randomForest")
1686163532266:model4 <- makeLearner("classif.svm")
1686163532266:model5 <- makeLearner("classif.naiveBayes")
1686163532266:# Train the models
1686163532267:trained_model1 <- train(model1, training_set)
1686163532267:trained_model2 <- train(model2, training_set)
1686163532268:trained_model3 <- train(model3, training_set)
1686163532268:trained_model4 <- train(model4, training_set)
1686163532268:trained_model5 <- train(model5, training_set)
1686163532268:# Make predictions on the testing set
1686163532269:predictions1 <- predict(trained_model1, newdata = testing_set)
1686163532269:predictions2 <- predict(trained_model2, newdata = testing_set)
1686163532269:predictions3 <- predict(trained_model3, newdata = testing_set)
1686163532270:predictions4 <- predict(trained_model4, newdata = testing_set)
1686163532270:predictions5 <- predict(trained_model5, newdata = testing_set)
1686163532270:# Evaluate the models
1686163532270:eval1 <- performance(predictions1, measures = list(acc))
1686163532271:eval2 <- performance(predictions2, measures = list(acc))
1686163532271:eval3 <- performance(predictions3, measures = list(acc))
1686163532271:eval4 <- performance(predictions4, measures = list(acc))
1686163532272:eval5 <- performance(predictions5, measures = list(acc))
1686163532272:performance <- data.frame(
1686163532272:Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
1686163532272:Accuracy = c(eval1$results$acc, eval2$results$acc, eval3$results$acc, eval4$results$acc, eval5$results$acc)
1686163532272:)
1686163532273:performance
1686163532273:# Classification
1686163532274:library(caret)
1686163532275:# Classification
1686163532275:library(caret)
1686163532275:library(caret)
1686163691656:install.packages("caret")
1686163873791:selected_df
1686163873999:selected_df
1686163913098:# Classification
1686163913100:library(caret)
1686163924066:# End the session
1686163924067:spark_disconnect(sc)
1686163999942:library(sparklyr)
1686163999943:library(dplyr)
1686163999944:# Install
1686163999944:spark_install()
1686163999962:# Connect
1686163999963:sc <- sparklyr::spark_connect(master = "local")
1686164003890:dfsetPath <- "/df/dfset.csv"
1686164003891:df <- spark_read_csv(sc, name = "my_df", path = dfsetPath, header = TRUE, infer_schema = TRUE)
1686164007037:# End the session
1686164007037:spark_disconnect(sc)
1686164053422:# End the session
1686164053423:spark_disconnect(sc)
1686164127467:library(sparklyr)
1686164127468:library(dplyr)
1686164127469:# Install
1686164127470:spark_install()
1686164127523:# Connect
1686164127524:sc <- sparklyr::spark_connect(master = "local")
1686164131272:datasetPath <- "/data/dataset.csv"
1686164131272:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164142071:# End the session
1686164142072:spark_disconnect(sc)
1686164143917:df
1686164267080:library(sparklyr)
1686164267080:library(dplyr)
1686164267081:# Install
1686164267082:spark_install()
1686164267125:# Connect
1686164267125:sc <- sparklyr::spark_connect(master = "local")
1686164269445:datasetPath <- "/data/dataset.csv"
1686164269445:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164270375:df
1686164271380:# End the session
1686164271380:spark_disconnect(sc)
1686164271381:spark_disconnect(sc)
1686164271383:library(sparklyr)
1686164271383:library(dplyr)
1686164271383:# Install
1686164271383:spark_install()
1686164271404:# Connect
1686164271404:sc <- sparklyr::spark_connect(master = "local")
1686164272838:datasetPath <- "/data/dataset.csv"
1686164272838:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164273846:# Display data
1686164273846:glimpse(df)
1686164274861:head(df)
1686164275876:# Exploring data
1686164275877:colnames(df)
1686164275879:summary(df)
1686164275881:# Select specific columns
1686164275881:selected_df <- df %>%
1686164275882:select(id, full_name, H, diameter)
1686164275884:selected_df
1686164276906:# Group by a column and compute aggregate functions
1686164276907:grouped_df <- df %>%
1686164276908:group_by(class) %>%
1686164276908:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686164276911:grouped_df
1686164277937:# Collect the resulting data to the local R environment
1686164277938:local_df <- collect(selected_df)
1686164278964:local_df
1686164278991:spark_disconnect(sc)
1686164278993:# Connect
1686164278994:sc <- sparklyr::spark_connect(master = "local")
1686164281394:# Connect
1686164281395:sc <- sparklyr::spark_connect(master = "local")
1686164283791:sc <- sparklyr::spark_connect(master = "local")
1686164288930:ddd
1686164293564:4 + 5
1686164304569:spark_disconnect(sc)
1686164310980:4-5
1686164335547:library(sparklyr)
1686164335549:library(dplyr)
1686164335549:# Install
1686164335550:spark_install()
1686164335571:# Connect
1686164335571:sc <- sparklyr::spark_connect(master = "local")
1686164338806:datasetPath <- "/data/dataset.csv"
1686164338806:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164349476:# Display data
1686164349477:glimpse(df)
1686164350196:head(df)
1686164350904:# Exploring data
1686164350904:colnames(df)
1686164350905:summary(df)
1686164350906:# Select specific columns
1686164350906:selected_df <- df %>%
1686164350907:select(id, full_name, H, diameter)
1686164350908:selected_df
1686164351211:# Group by a column and compute aggregate functions
1686164351211:grouped_df <- df %>%
1686164351212:group_by(class) %>%
1686164351212:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686164351214:grouped_df
1686164351876:# Collect the resulting data to the local R environment
1686164351876:local_df <- collect(selected_df)
1686164355056:local_df
1686164355066:spark_disconnect(sc)
1686164848637:library(sparklyr)
1686164848639:library(dplyr)
1686164848639:# Install
1686164848639:spark_install()
1686164848658:# Connect
1686164848658:sc <- sparklyr::spark_connect(master = "local")
1686164851894:datasetPath <- "/data/dataset.csv"
1686164851894:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164862896:df
1686164863678:# End the session
1686164863679:spark_disconnect(sc)
1686165332601:library(sparklyr)
1686165332602:library(dplyr)
1686165332603:# Install
1686165332603:spark_install()
1686165332624:# Connect
1686165332625:sc <- sparklyr::spark_connect(master = "local")
1686165335767:datasetPath <- "/data/dataset.csv"
1686165335767:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686165347792:# Sanity
1686165347792:glimpse(df)
1686165348561:# Comparison of classification models with different parameter values
1686165348561:model_params <- list(
1686165348562:list(param1 = value1),
1686165348562:list(param1 = value2),
1686165348562:...
1686165348562:)
1686165348563:# Define a function to create and evaluate classification models
1686165348563:evaluate_classification_model <- function(params) {
1686165348564:# Create the classification model with the specified parameter values
1686165348564:model <- spark.ml.function_to_create_model(params)
1686165348564:# Split the data into training and testing sets
1686165348564:splits <- randomSplit(df, c(0.7, 0.3), seed = 123)
1686165348564:training_set <- splits[[1]]
1686165348565:testing_set <- splits[[2]]
1686165348565:# Fit the model on the training set
1686165348565:trained_model <- ml.fit(model, training_set)
1686165348565:# Make predictions on the testing set
1686165348566:predictions <- ml.transform(trained_model, testing_set)
1686165348566:# Calculate performance metrics (e.g., accuracy, precision, recall, F1-score)
1686165348566:performance <- ml.function_to_calculate_performance(predictions)
1686165348567:# Return the performance metrics
1686165348567:return(performance)
1686165348567:}
1686165348568:# Perform comparison of classification models with different parameter values
1686165348568:for (params in model_params) {
1686165348568:performance <- evaluate_classification_model(params)
1686165348568:print(performance)
1686165348568:}
1686165348571:# Comparison of classification models with different types
1686165348571:model_types <- list("model_type1", "model_type2", ...)
1686165348571:# Define a function to create and evaluate classification models
1686165348572:evaluate_classification_model_type <- function(model_type) {
1686165348572:# Create the classification model of the specified type
1686165348572:model <- spark.ml.function_to_create_model_type(model_type)
1686165348572:# Split the data into training and testing sets
1686165348572:splits <- randomSplit(df, c(0.7, 0.3), seed = 123)
1686165348573:training_set <- splits[[1]]
1686165348573:testing_set <- splits[[2]]
1686165348573:# Fit the model on the training set
1686165348573:trained_model <- ml.fit(model, training_set)
1686165348574:# Make predictions on the testing set
1686165348574:predictions <- ml.transform(trained_model, testing_set)
1686165348574:# Calculate performance metrics (e.g., accuracy, precision, recall, F1-score)
1686165348575:performance <- ml.function_to_calculate_performance(predictions)
1686165348575:# Return the performance metrics
1686165348576:return(performance)
1686165348576:}
1686165348576:# Perform comparison of classification models with different types
1686165348577:for (model_type in model_types) {
1686165348577:performance <- evaluate_classification_model_type(model_type)
1686165348577:print(performance)
1686165348577:}
1686165348580:# Clustering analysis with different parameter values
1686165348580:clustering_params <- list(
1686165348580:list(param1 = value1),
1686165348580:list(param1 = value2),
1686165348580:...
1686165348580:)
1686165348581:# Define a function to perform clustering analysis
1686165348581:perform_clustering <- function(params) {
1686165348581:# Perform clustering with the specified parameter values
1686165348581:clusters <- spark.ml.function_to_perform_clustering(params)
1686165348582:# Analyze and interpret the obtained clusters
1686165348582:analysis <- spark.ml.function_to_analyze_clusters(clusters)
1686165348582:# Return the analysis results
1686165348583:return(analysis)
1686165348583:}
1686165348583:# Perform clustering analysis with different parameter values
1686165348583:for (params in clustering_params) {
1686165348584:analysis <- perform_clustering(params)
1686165348584:print(analysis)
1686165348584:}
1686165348586:# Generate the analytical report
1686165348586:report <- create_report(df, model_params, model_types, clustering_params)
1686165348586:save_report(report, "report.pdf")
1686165348587:# End the session
1686165348587:spark_disconnect(sc)
1686165639283:library(sparklyr)
1686165639285:library(dplyr)
1686165639286:# Install
1686165639286:#spark_install()
1686165639287:# Connect
1686165639287:sc <- sparklyr::spark_connect(master = "local")
1686165642450:datasetPath <- "/data/dataset.csv"
1686165642450:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686165653528:# Sanity
1686165653528:glimpse(df)
1686165654246:# End the session
1686165654246:spark_disconnect(sc)
1689160715882:library(sparklyr)
1689160716890:library(dplyr)
1689160716926:# Install
1689160716927:spark_install()
1689160717480:# Connect
1689160717480:sc <- sparklyr::spark_connect(master = "local")
1689160723003:datasetPath <- "/data/dataset.csv"
1689160723004:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689160735584:# Display data
1689160735584:glimpse(df)
1689160736356:head(df)
1689160736985:# Exploring data
1689160736985:colnames(df)
1689160736986:summary(df)
1689160736987:# Select specific columns
1689160736987:selected_df <- df %>%
1689160736987:select(id, full_name, H, diameter)
1689160736990:selected_df
1689160737286:# Group by a column and compute aggregate functions
1689160737286:grouped_df <- df %>%
1689160737286:group_by(class) %>%
1689160737287:summarise(mean_H = mean(H), max_diameter = max(diameter))
1689160737290:grouped_df
1689160738040:# Collect the resulting data to the local R environment
1689160738040:local_df <- collect(selected_df)
1689160741783:local_df
1689160741793:spark_disconnect(sc)
1689160916482:# Connect
1689160916484:sc <- sparklyr::spark_connect(master = "local")
1689160922028:datasetPath <- "/data/dataset.csv"
1689160922028:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689160934091:# Sanity
1689160934091:glimpse(df)
1689161687334:df_cleaned <- df %>%
1689161687335:filter(!is.na(column_name))
1689161701932:glimpse(df_cleaned)
1689161936868:install.packages("tidyverse")
1689162016901:install.packages("tidyverse", repos = "https://cran.rstudio.com/")
1689162396737:library(tidyr)
1689162408758:install.packages("gridExtra")
1689162434182:install.packages("gridExtra", repos = "https://cran.rstudio.com/")
1689162482052:install.packages("kableExtra", repos = "https://cran.rsduio.com/")
1689162507508:install.packages("kableExtra", repos = "https://cran.rsdtuio.com/")
1689162558565:library(sparklyr)
1689162559088:library(dplyr)
1689162559103:library(tidyr)
1689162559103:library(ggplot2)
1689162559460:library(magrittr)
1689162559472:library(knitr)
1689162559501:library(kableExtra)
1689162559503:library(cowplot)
1689162601574:# Install
1689162601592:spark_install()
1689162614129:# Sanity
1689162614147:glimpse(df)
1689162625814:# End the session
1689162625815:spark_disconnect(sc)
1689162631555:# Connect
1689162631557:sc <- sparklyr::spark_connect(master = "local")
1689162645844:datasetPath <- "/data/dataset.csv"
1689162648445:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689162664391:# Sanity
1689162664392:glimpse(df)
1689162691037:# Columns
1689162691037:colnames(df)
1689162821513:df <- df %>%
1689162821515:mutate(abs_mag_category = case_when(
1689162821516:H < 15 ~ "Very Bright",
1689162821516:H >= 15 & H < 20 ~ "Moderately Bright",
1689162821517:TRUE ~ "Dim"
1689162821517:))
1689162840213:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689162840231:df <- df %>%
1689162840231:mutate(orbit_class_category = case_when(
1689162840232:startsWith(orbit_id, "M") ~ "Main Belt",
1689162840232:startsWith(orbit_id, "O") ~ "Outer Solar System",
1689162840233:startsWith(orbit_id, "I") ~ "Inner Solar System",
1689162840234:TRUE ~ "Unknown"
1689162840234:))
1689162844484:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689162844485:df <- df %>%
1689162844486:mutate(size_category = case_when(
1689162844486:diameter < 1 ~ "Small",
1689162844487:diameter >= 1 & diameter <= 10 ~ "Medium",
1689162844488:diameter > 10 ~ "Large",
1689162844488:TRUE ~ "Unknown"
1689162844489:))
1689162867839:# Sanity columns
1689162867841:colnames(df)
1689167979628:library(sparklyr)
1689167988155:library(dplyr)
1689167991487:library(tidyr)
1689167995119:library(ggplot2)
1689167997098:library(magrittr)
1689167998673:library(knitr)
1689168006647:library(sparklyr)
1689168016855:# Install
1689168016857:spark_install()
1689168056174:install.packages("sparklyr")
1689168089419:install.packages("sparklyr", repos="https://cran.rstudio.com")
1689168145466:library(sparklyr)
1689168260753:# Install
1689168260755:spark_install()
1689168269279:# Install
1689168269297:spark_install()
1689168284860:sc <- sparklyr::spark_connect(master = "local")
1689168295985:datasetPath <- "/data/dataset.csv"
1689168300126:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689168316633:# Columns
1689168316634:colnames(df)
1689168465443:# Filter data
1689168465444:df <- df %>%
1689168465445:filter(!is.na(diameter_sigma))
1689168471032:# Exclude the "prefix" column
1689168471033:df <- select(df, -prefix)
1689168490258:# Columns sanity
1689168490260:colnames(df)
1689168500835:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689168500837:df <- df %>%
1689168500837:mutate(abs_mag_category = case_when(
1689168500838:H < 15 ~ "Very Bright",
1689168500838:H >= 15 & H < 20 ~ "Moderately Bright",
1689168500839:TRUE ~ "Dim"
1689168500839:))
1689168502344:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689168502345:df <- df %>%
1689168502346:mutate(orbit_class_category = case_when(
1689168502346:startsWith(orbit_id, "M") ~ "Main Belt",
1689168502347:startsWith(orbit_id, "O") ~ "Outer Solar System",
1689168502347:startsWith(orbit_id, "I") ~ "Inner Solar System",
1689168502348:TRUE ~ "Unknown"
1689168502348:))
1689168513367:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689168513368:df <- df %>%
1689168513369:mutate(size_category = case_when(
1689168513369:diameter < 1 ~ "Small",
1689168513370:diameter >= 1 & diameter <= 10 ~ "Medium",
1689168513370:diameter > 10 ~ "Large",
1689168513371:TRUE ~ "Unknown"
1689168513371:))
1689168547390:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689168547392:df <- df %>%
1689168547392:mutate(orbit_class_category = case_when(
1689168547393:"M" ~ "Main Belt",
1689168547393:"O" ~ "Outer Solar System",
1689168547394:"I" ~ "Inner Solar System",
1689168547394:TRUE ~ "Unknown"
1689168547395:))
1689168634038:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689168634040:df <- df %>%
1689168634040:mutate(abs_mag_category = case_when(
1689168634040:H < 15 ~ "Very Bright",
1689168634041:H >= 15 & H < 20 ~ "Moderately Bright",
1689168634041:TRUE ~ "Dim"
1689168634041:))
1689168636299:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689168636301:df <- df %>%
1689168636301:mutate(size_category = case_when(
1689168636302:diameter < 1 ~ "Small",
1689168636302:diameter >= 1 & diameter <= 10 ~ "Medium",
1689168636302:diameter > 10 ~ "Large",
1689168636303:TRUE ~ "Unknown"
1689168636303:))
1689168639370:# Sanity columns
1689168639371:colnames(df)
1689168907296:# Classification modeling
1689168907298:# Regression
1689168907298:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H", "diameter"))
1689168930285:# Classification modeling
1689168930287:# Regression
1689168930288:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689168974091:# Data split
1689168974093:df_split <- sdf_partition(df, training = 0.7, test = 0.3)
1689168989560:# Data split
1689168989578:df_split <- df_random_split(df, training = 0.7, test = 0.3)
1689169003466:# Data split
1689169003467:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169010706:df_split
1689169400789:# Data split
1689169400790:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169402257:# Data split
1689169402259:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169463589:result <- ml_evaluate(m1, data_split$test)
1689169481165:m1 <- ml_fit(model1, data_split$training)
1689169490379:m1 <- ml_fit(model1, df_split$training)
1689169600666:df_split <- sdf_with_column(df_split, "H", cast(col("H"), "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689169691436:# Data split
1689169691438:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169696149:# Check the split out
1689169696151:df_split
1689170214265:# Type conversion for H
1689170214266:df <- df %>%
1689170214267:mutate(H = cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170230534:# Regression
1689170230536:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689170356980:# Type conversion for H
1689170356982:df <- df %>%
1689170356982:mutate(H = cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170419817:# Type conversion for H
1689170419819:df <- df %>%
1689170419820:select(-H) %>%
1689170419820:mutate(H = cast(col("H"), "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>")) %>%
1689170419821:select(H, everything())
1689170468663:# Type conversion for H
1689170468664:df <- df %>%
1689170468664:mutate(H = alias(H, cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>")))
1689170526184:# Type conversion for H
1689170526186:df <- df %>%
1689170526186:mutate(H = `H` %>% cast("struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170565663:# Type conversion for H
1689170565664:df <- df %>%
1689170565665:mutate("H" =  "H" %>% cast("struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170579598:# Data split
1689170579600:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689170589489:# Check the split out
1689170589491:df_split
1689170601075:# Regression
1689170601076:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689170614326:View(df_split)
1689170617941:View(model1)
1689170646156:# Traing the model
1689170646157:m1 <- ml_fit(model1, df_split$training)
1689170674716:# Regression
1689170674717:model1 <- ml_logistic_regression(sc, label_col = "H", features_col = c("albedo"))
1689170677475:# Traing the model
1689170677477:m1 <- ml_fit(model1, df_split$training)
1689170869358:# Regression
1689170869360:model1 <- ml_logistic_regression(sc, label_col = "epoch", features_col = c("epoch_mjd"))
1689170872214:# Traing the model
1689170872216:m1 <- ml_fit(model1, df_split$training)
1689171156949:# Show data
1689171156950:kable(head(df, n=10L),
1689171156951:col.names = c("diameter",
1689171156951:"H",
1689171156952:"albedo",
1689171156952:"abs_mag_category",
1689171156953:"size_category"),
1689171156953:label = "Table view of filtered data",
1689171156954:format = "html",
1689171156954:align = "ccccc"
1689171156955:)
1689171211241:# Show data
1689171211243:kable(head(df, n=10L),
1689171211243:col.names = c("diameter",
1689171211244:"H",
1689171211244:"albedo",
1689171211245:"abs_mag_category",
1689171211245:"size_category"),
1689171211245:label = "Table view of filtered data",
1689171211246:format = "html",
1689171211246:align = "ccccc"
1689171211247:)
1689171312754:# Show data
1689171312756:kable(head(df, n = 10L),
1689171312756:col.names = c("diameter",
1689171312757:"H",
1689171312757:"albedo",
1689171312758:"abs_mag_category",
1689171312758:"size_category"),
1689171312758:caption = "Table view of filtered data",
1689171312759:format = "html",
1689171312759:align = "c"
1689171312760:)
1689171361005:# Show data
1689171361007:kable(head(df, n = 10L),
1689171361007:col.names = colnames(df),
1689171361008:caption = "Table view of filtered data",
1689171361008:format = "html",
1689171361009:align = "c"
1689171361009:)
1689171634004:# Regression
1689171634006:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689171636962:# Traing the model
1689171636964:m1 <- ml_fit(model1, df_split$training)
1689171715142:# Check the split out
1689171715143:df_split
1689171720791:View(model1)
1689171727458:# Regression
1689171727460:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689171737974:# Regression
1689171737976:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter", "H"))
1689171748358:rlang::last_trace(
1689171750022:)
1689171995050:# Show data
1689171995052:kable(head(df, n = 10L),
1689171995052:col.names = colnames(df),
1689171995053:caption = "Table view of filtered data",
1689171995053:format = "html",
1689171995054:align = "c"
1689171995054:) %>%
1689171995055:kableExtra::kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16)
1689172960644:spark_disconnect(df_spark)
1689172981279:# Show data
1689172981280:kable(head(df, n = 10L),
1689172981281:col.names = colnames(df),
1689172981281:caption = "Table view of filtered data",
1689172981282:format = "html",
1689172981282:align = "c"
1689172981283:)
1689173072608:# Regression
1689173072610:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter", "H"))
1689173079819:# Regression
1689173079820:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689173082396:# Traing the model
1689173082397:m1 <- ml_fit(model1, df_split$training)
1689173087672:# Regression
1689173087674:model1 <- ml_logistic_regression(df, label_col = "albedo", features_col = c("diameter"))
1689173543074:slelect(albedo, H, diameter, full_name)
1689173547741:select(albedo, H, diameter, full_name)
1689173586870:df %>% select(albedo, H, diameter, full_name)
1689173601631:df
1689173655048:# Select data for further testing
1689173655050:dataset <- df %>% select(albedo, H, diameter, full_name)
1689173667661:# Data split
1689173667679:df_split <- sdf_random_split(dataset, training = 0.7, test = 0.3)
1689173670843:# Check the split out
1689173670845:df_split
1689173714238:# Filter data
1689173714240:df <- df %>%
1689173714240:filter(!is.na(diameter_sigma)||
1689173714241:!is.na(albedo))
1689173728525:# Filter data
1689173728527:df <- df %>%
1689173728527:filter(!is.na(diameter_sigma)||
1689173728528:!is.na(albedo))
1689173734497:# Exclude the "prefix" column
1689173734499:df <- select(df, -prefix)
1689173756604:# Select data for further testing
1689173756605:dataset <- df %>% select(albedo, H, diameter, full_name)
1689173760114:# Data split
1689173760115:df_split <- sdf_random_split(dataset, training = 0.7, test = 0.3)
1689173762139:# Check the split out
1689173762141:df_split
1689173807029:# Formula
1689173807031:formula <- (1329 * 10 ^ (-H/5))/diameter)^2
1689173818740:# Formula
1689173818741:formula <- ((1329 * 10 ^ (-H/5))/diameter)^2)
1689173827862:# Formula
1689173827862:formula <- ((1329 * 10 ^(-H/5))/diameter)^2)
1689174346812:# End the session
1689174346813:spark_disconnect(sc)
1689174354058:View(dataset)
1689174414011:# Install
1689174414012:spark_install()
1689174420816:# Connect
1689174420817:sc <- sparklyr::spark_connect(master = "local")
1689174473986:# Connect
1689174473988:sc <- sparklyr::spark_connect(master = "local")
1689174484423:datasetPath <- "/data/dataset.csv"
1689174486582:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689174653775:# Filter data
1689174653776:df <- df %>%
1689174653777:filter(!is.na(diameter_sigma) ||
1689174653777:!is.na(albedo) ||
1689174653777:!is.na(H) ||
1689174653777:!is.na(diameter)||
1689174653778:!is.na(epoch) ||
1689174653778:!is.na(full_name)
1689174653778:)
1689174659318:View(df)
1689174670485:# Columns sanity
1689174670486:colnames(df)
1689174695166:df
1689174839488:# Create a new column with H divided by 5
1689174839490:df <- df %>%
1689174839491:mutate(new_column = withColumn("new_column", df$H / 5))
1689174855724:# Create a new column with H divided by 5
1689174855726:df <- df %>%
1689174855726:mutate(new_column = withColumn("H/5", df$H / 5))
1689174925127:# Create a new column with H divided by 5
1689174925129:df <- df %>%
1689174925129:mutate(H/5 = H / 5)
1689174935521:# Create a new column with H divided by 5
1689174935522:df <- df %>%
1689174935523:mutate(H/5 ~ H / 5)
1689174942529:# Create a new column with H divided by 5
1689174942529:df <- df %>%
1689174942530:mutate(H5 ~ H / 5)
1689174953465:# Create a new column with H divided by 5
1689174953467:df <- df %>%
1689174953467:mutate(new_column ~ H / 5)
1689174975088:# Create a new column with H divided by 5
1689174975090:df <- df %>%
1689174975090:mutate(new_column = H / 5)
1689174989011:colnames(df)
1689175055287:rename(new_column = H5)
1689175066043:rename(new_column = "h_5")
1689175071971:rename(new_column = h_5)
1689175089588:rename(new_column = h)
1689175116470:# Create a new column with H divided by 5 (latter used in the formula)
1689175116472:df <- df %>%
1689175116472:mutate(new_column = H / 5) %>%
1689175116473:rename(new_column = h)
1689175133919:# Create a new column with H divided by 5 (latter used in the formula)
1689175133920:df <- df %>%
1689175133921:mutate(new_column = H / 5) %>%
1689175133921:rename(h = new_column)
1689175142071:colnames(df)
1689175150839:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175150841:df <- df %>%
1689175150842:mutate(abs_mag_category = case_when(
1689175150842:H < 15 ~ "Very Bright",
1689175150843:H >= 15 & H < 20 ~ "Moderately Bright",
1689175150843:TRUE ~ "Dim"
1689175150844:))
1689175186165:# Create a new column with H divided by 5 (latter used in the formula)
1689175186183:df <- df %>%
1689175186184:mutate(new_column = H / 5) %>%
1689175186184:rename(H5 = new_column)
1689175193123:# Create a new column with H divided by 5 (latter used in the formula)
1689175193124:df <- df %>%
1689175193125:mutate(new_column = H / 5) %>%
1689175193125:rename(H5 = h)
1689175208710:# Create a new column with H divided by 5 (latter used in the formula)
1689175208712:df <- df %>%
1689175208712:mutate(new_column = H / 5) %>%
1689175208713:rename(h = new_column)
1689175215501:colnames(h)
1689175221598:colnames(df)
1689175229917:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175229918:df <- df %>%
1689175229919:mutate(abs_mag_category = case_when(
1689175229920:H < 15 ~ "Very Bright",
1689175229920:H >= 15 & H < 20 ~ "Moderately Bright",
1689175229921:TRUE ~ "Dim"
1689175229921:))
1689175249142:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175249144:df <- df %>%
1689175249145:mutate(abs_mag_category = case_when(
1689175249145:df$H < 15 ~ "Very Bright",
1689175249146:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175249146:TRUE ~ "Dim"
1689175249147:))
1689175259887:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175259888:df <- df %>%
1689175259889:mutate(abs_mag_category = case_when(
1689175259889:H < 15 ~ "Very Bright",
1689175259890:H >= 15 & H < 20 ~ "Moderately Bright",
1689175259890:TRUE ~ "Dim"
1689175259891:))
1689175320494:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175320495:df <- df %>%
1689175320496:mutate(size_category = case_when(
1689175320496:diameter < 1 ~ "Small",
1689175320497:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175320498:diameter > 10 ~ "Large",
1689175320498:TRUE ~ "Unknown"
1689175320499:))
1689175331327:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175331328:df <- df %>%
1689175331329:mutate(abs_mag_category = case_when(
1689175331329:h < 15 ~ "Very Bright",
1689175331330:h >= 15 & h < 20 ~ "Moderately Bright",
1689175331330:TRUE ~ "Dim"
1689175331331:))
1689175367379:df <- df %>% rename(H5 = h)
1689175376946:colnames(df
1689175376948:)
1689175393547:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175393548:df <- df %>%
1689175393548:mutate(abs_mag_category = case_when(
1689175393549:H < 15 ~ "Very Bright",
1689175393549:H >= 15 & H < 20 ~ "Moderately Bright",
1689175393550:TRUE ~ "Dim"
1689175393550:))
1689175419877:# Create a new column with H divided by 5 (latter used in the formula)
1689175419879:df <- df %>%
1689175419880:rename(h = H5)
1689175452951:# End the session
1689175452953:spark_disconnect(sc)
1689175465582:# Connect
1689175465584:sc <- sparklyr::spark_connect(master = "local")
1689175491032:datasetPath <- "/data/dataset.csv"
1689175491773:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175519948:# Filter data
1689175519950:df <- df %>%
1689175519951:filter(!is.na(diameter_sigma) ||
1689175519951:!is.na(albedo) ||
1689175519952:!is.na(H) ||
1689175519952:!is.na(diameter)||
1689175519953:!is.na(epoch) ||
1689175519953:!is.na(full_name)
1689175519954:)
1689175524359:# Create a new column with H divided by 5 (latter used in the formula)
1689175524360:df <- df %>%
1689175524361:mutate(new_column = H / 5) %>%
1689175524361:rename(h = new_column)
1689175537270:colnames(df)
1689175550156:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175550158:df <- df %>%
1689175550158:mutate(abs_mag_category = case_when(
1689175550159:H < 15 ~ "Very Bright",
1689175550160:H >= 15 & H < 20 ~ "Moderately Bright",
1689175550160:TRUE ~ "Dim"
1689175550161:))
1689175575570:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175575572:df <- df %>%
1689175575572:mutate(abs_mag_category = case_when(
1689175575573:df$H < 15 ~ "Very Bright",
1689175575573:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175575574:TRUE ~ "Dim"
1689175575575:))
1689175597377:df
1689175605041:df
1689175637242:# End the session
1689175637243:spark_disconnect(sc)
1689175650731:# Connect
1689175650749:sc <- sparklyr::spark_connect(master = "local")
1689175660200:datasetPath <- "/data/dataset.csv"
1689175660203:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175669188:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175680389:# Filter data
1689175680391:df <- df %>%
1689175680391:filter(!is.na(diameter_sigma) ||
1689175680392:!is.na(albedo) ||
1689175680392:!is.na(H) ||
1689175680393:!is.na(diameter)||
1689175680393:!is.na(epoch) ||
1689175680394:!is.na(full_name)
1689175680394:)
1689175684389:df
1689175744492:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175744494:df <- df %>%
1689175744494:mutate(abs_mag_category = case_when(
1689175744495:df$H < 15 ~ "Very Bright",
1689175744495:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175744496:TRUE ~ "Dim"
1689175744496:))
1689175758566:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175758568:df <- df %>%
1689175758568:mutate(abs_mag_category = case_when(
1689175758569:H < 15 ~ "Very Bright",
1689175758570:H >= 15 & H < 20 ~ "Moderately Bright",
1689175758570:TRUE ~ "Dim"
1689175758571:))
1689175761525:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175761526:df <- df %>%
1689175761527:mutate(size_category = case_when(
1689175761527:diameter < 1 ~ "Small",
1689175761528:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175761528:diameter > 10 ~ "Large",
1689175761529:TRUE ~ "Unknown"
1689175761529:))
1689175763582:# Sanity columns
1689175763583:colnames(df)
1689175879010:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175879012:df <- df %>%
1689175879013:mutate(brightness = case_when(
1689175879013:H < 15 ~ "Very Bright",
1689175879014:H >= 15 & H < 20 ~ "Moderately Bright",
1689175879014:TRUE ~ "Dim"
1689175879015:))
1689175894549:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175894550:df <- df %>%
1689175894551:mutate(size_category = case_when(
1689175894551:diameter < 1 ~ "Small",
1689175894552:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175894552:diameter > 10 ~ "Large",
1689175894553:TRUE ~ "Unknown"
1689175894553:))
1689175912355:# Select most relevant data for further analising
1689175912356:df <- df %>% select(albedo, H, diameter, diameter_sigma, epoch, full_name, brightness, size_category)
1689175918599:# Show data
1689175918600:kable(head(df, n = 10L),
1689175918601:col.names = colnames(df),
1689175918602:caption = "Table view of filtered data",
1689175918602:format = "html",
1689175918603:align = "c"
1689175918603:)
1689176018963:# Select most relevant data for further analising
1689176018965:df <- df %>% select(name, albedo, H, diameter, diameter_sigma, epoch, brightness, size_category)
1689176023989:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689176031327:# Filter data
1689176031327:df <- df %>%
1689176031328:filter(!is.na(diameter_sigma) ||
1689176031328:!is.na(albedo) ||
1689176031329:!is.na(H) ||
1689176031329:!is.na(diameter)||
1689176031330:!is.na(epoch) ||
1689176031330:!is.na(name)
1689176031331:)
1689176034039:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689176034042:df <- df %>%
1689176034043:mutate(brightness = case_when(
1689176034044:H < 15 ~ "Very Bright",
1689176034044:H >= 15 & H < 20 ~ "Moderately Bright",
1689176034045:TRUE ~ "Dim"
1689176034045:))
1689176036548:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689176036549:df <- df %>%
1689176036550:mutate(size_category = case_when(
1689176036550:diameter < 1 ~ "Small",
1689176036551:diameter >= 1 & diameter <= 10 ~ "Medium",
1689176036551:diameter > 10 ~ "Large",
1689176036552:TRUE ~ "Unknown"
1689176036552:))
1689176039348:# Sanity columns
1689176039350:colnames(df)
1689176088975:# H devided by 5 (needed for formula)
1689176088976:df <- df %>%
1689176088977:mutate(new_column = H / 5)
1689176090750:# Sanity columns
1689176090752:colnames(df)
1689176094453:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689176094455:df <- df %>%
1689176094455:mutate(size_category = case_when(
1689176094456:diameter < 1 ~ "Small",
1689176094456:diameter >= 1 & diameter <= 10 ~ "Medium",
1689176094457:diameter > 10 ~ "Large",
1689176094457:TRUE ~ "Unknown"
1689176094458:))
1689176096669:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689176096669:df <- df %>%
1689176096670:mutate(brightness = case_when(
1689176096670:H < 15 ~ "Very Bright",
1689176096671:H >= 15 & H < 20 ~ "Moderately Bright",
1689176096672:TRUE ~ "Dim"
1689176096672:))
1689176108932:# H devided by 5 (needed for formula)
1689176108932:df <- df %>%
1689176108933:mutate(H_devided = H / 5)
1689176111495:# Sanity columns
1689176111497:colnames(df)
1689176123399:# Select most relevant data for further analising
1689176123400:df <- df %>% select(name, albedo, H, H_devided, diameter, diameter_sigma, epoch, brightness, size_category)
1689176125899:# Show selected data
1689176125901:kable(head(df, n = 10L),
1689176125902:col.names = colnames(df),
1689176125902:caption = "Table view of filtered data",
1689176125903:format = "html",
1689176125903:align = "c"
1689176125904:)
1689176258705:# Data split
1689176258706:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689176262159:# Check the split out
1689176262160:df_split
1689176358555:# Check the split out
1689176358556:print(df_split, n=100)
1689176367643:# Check the split out
1689176367645:print(df_split,100)
1689176379186:# Check the split out
1689176379186:df
1689176424894:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689176447768:# Filter data
1689176447769:df <- df %>%
1689176447769:filter(!(is.na(diameter_sigma) ||
1689176447770:is.na(albedo) ||
1689176447770:is.na(H) ||
1689176447771:is.na(diameter)||
1689176447771:is.na(epoch) ||
1689176447772:is.na(name)
1689176447772:))
1689176450705:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689176450706:df <- df %>%
1689176450707:mutate(brightness = case_when(
1689176450708:H < 15 ~ "Very Bright",
1689176450708:H >= 15 & H < 20 ~ "Moderately Bright",
1689176450709:TRUE ~ "Dim"
1689176450709:))
1689176453708:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689176453710:df <- df %>%
1689176453710:mutate(size_category = case_when(
1689176453711:diameter < 1 ~ "Small",
1689176453711:diameter >= 1 & diameter <= 10 ~ "Medium",
1689176453712:diameter > 10 ~ "Large",
1689176453712:TRUE ~ "Unknown"
1689176453713:))
1689176457546:# H devided by 5 (needed for formula)
1689176457546:df <- df %>%
1689176457547:mutate(H_devided = H / 5)
1689176478494:# H devided by 5 (needed for formula)
1689176478494:df <- df %>%
1689176478495:mutate(H_divd = H / 5)
1689176480718:# Sanity columns
1689176480719:colnames(df)
1689176483871:# Select most relevant data for further analising
1689176483873:df <- df %>% select(name, albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689176487515:# Show selected data
1689176487516:kable(head(df, n = 10L),
1689176487517:col.names = colnames(df),
1689176487517:caption = "Table view of filtered data",
1689176487518:format = "html",
1689176487518:align = "c"
1689176487518:)
1689176491049:# Data split
1689176491049:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689176493781:# Check the split out
1689176493784:df
1689176498809:# Check the split out
1689176498811:df_split
1689176515247:# Formula
1689176515249:formula <- ((1329 * 10 ^(-H_divd))/diameter)^2
1689176526563:# Formula
1689176526565:formula <- ((1329 * 10 ^(-H))/diameter)^2
1689176557716:# Formula
1689176557717:formula <- ((1329 * 10 ^(-df$H_divd))/df$diameter)^2
1689176565902:# Check the split out
1689176565904:df_split
1689176618556:# Data split
1689176618558:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689176621030:# Check the split out
1689176621031:df_split
1689176657874:# Formula
1689176657876:formula <- albedo ~ ((1329 * 10 (-H_divd))/diameter)^2
1689176719514:# Regression
1689176719515:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689176823536:# Formula
1689176823536:formula <- albedo ~ I(((1329 * 10^(-H_divd))/diameter)^2)
1689176826120:# Regression
1689176826122:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689176845739:# Regression
1689176845741:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689176850817:# Formula
1689176850818:formula <- albedo ~ H_divd * diameter
1689176852537:# Regression
1689176852538:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689177079839:# Formula
1689177079840:formula <- albedo ~ H_divd + diameter
1689177083200:# Regression
1689177083202:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689181183969:colnames(df)
1689181191287:# Show selected data
1689181191288:kable(head(df, n = 10L),
1689181191289:col.names = colnames(df),
1689181191289:caption = "Table view of filtered data",
1689181191290:format = "html",
1689181191290:align = "c"
1689181191291:)
1689181525439:df$albedo <- (df$albedo - min(df$albedo)) / (max(df$albedo) - min(df$albedo))
1689181740281:# Regression
1689181740283:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689181858820:df <- na.omit(df)
1689181861418:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689181870970:df <- na.omit(df)
1689181882219:df
1689181893032:head(df)
1689181943355:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689181951110:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689181951112:df <- df %>%
1689181951112:mutate(brightness = case_when(
1689181951113:H < 15 ~ "Very Bright",
1689181951113:H >= 15 & H < 20 ~ "Moderately Bright",
1689181951114:TRUE ~ "Dim"
1689181951114:))
1689181953753:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689181953754:df <- df %>%
1689181953755:mutate(size_category = case_when(
1689181953756:diameter < 1 ~ "Small",
1689181953756:diameter >= 1 & diameter <= 10 ~ "Medium",
1689181953757:diameter > 10 ~ "Large",
1689181953757:TRUE ~ "Unknown"
1689181953758:))
1689181956535:# H devided by 5 (needed for formula)
1689181956537:df <- df %>%
1689181956538:mutate(H_divd = H / 5)
1689181958545:# Sanity columns
1689181958546:colnames(df)
1689181964113:# Select most relevant data for further analising
1689181964114:df <- df %>% select(name, albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689181979411:colnames(df)
1689181991662:df <- na.omit(df)
1689182006716:head(df)
1689182014054:# Show selected data
1689182014056:kable(head(df, n = 10L),
1689182014056:col.names = colnames(df),
1689182014057:caption = "Table view of filtered data",
1689182014057:format = "html",
1689182014058:align = "c"
1689182014058:)
1689182091690:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689182102392:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689182102393:df <- df %>%
1689182102394:mutate(brightness = case_when(
1689182102394:H < 15 ~ "Very Bright",
1689182102395:H >= 15 & H < 20 ~ "Moderately Bright",
1689182102395:TRUE ~ "Dim"
1689182102396:))
1689182104532:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689182104533:df <- df %>%
1689182104533:mutate(size_category = case_when(
1689182104534:diameter < 1 ~ "Small",
1689182104534:diameter >= 1 & diameter <= 10 ~ "Medium",
1689182104535:diameter > 10 ~ "Large",
1689182104535:TRUE ~ "Unknown"
1689182104536:))
1689182107499:# H devided by 5 (needed for formula)
1689182107501:df <- df %>%
1689182107501:mutate(H_divd = H / 5)
1689182117601:# Select most relevant data for further analising
1689182117602:df <- df %>% select(name, albedo, H, H_divd, diameter, epoch, brightness, size_category)
1689182119770:df <- na.omit(df)
1689182148582:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689182155335:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689182155337:df <- df %>%
1689182155337:mutate(brightness = case_when(
1689182155338:H < 15 ~ "Very Bright",
1689182155338:H >= 15 & H < 20 ~ "Moderately Bright",
1689182155339:TRUE ~ "Dim"
1689182155339:))
1689182156706:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689182156708:df <- df %>%
1689182156708:mutate(size_category = case_when(
1689182156709:diameter < 1 ~ "Small",
1689182156709:diameter >= 1 & diameter <= 10 ~ "Medium",
1689182156710:diameter > 10 ~ "Large",
1689182156710:TRUE ~ "Unknown"
1689182156711:))
1689182157957:# H devided by 5 (needed for formula)
1689182157958:df <- df %>%
1689182157959:mutate(H_divd = H / 5)
1689182200424:# Select most relevant data for further analising
1689182200425:dff <- df %>% select(name, albedo, H, H_divd, diameter, brightness, size_category)
1689182201862:dff <- na.omit(dff)
1689182239845:# Select most relevant data for further analising
1689182239847:dff <- df %>% select(name, albedo, H, H_divd, diameter_sigma, epoch, brightness, size_category)
1689182242262:dff <- na.omit(dff)
1689182253095:dff <- na.omit(dff)
1689182257283:# Select most relevant data for further analising
1689182257301:dff <- df %>% select(name, albedo, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182260239:dff <- na.omit(dff)
1689182282057:# Select most relevant data for further analising
1689182282059:dff <- df %>% select(name, albedo, H, H_divd, diameter, diameter_sigma, epoch)
1689182284123:dff <- na.omit(dff)
1689182295411:# Select most relevant data for further analising
1689182295412:dff <- df %>% select(name, albedo, H, H_divd, diameter, diameter_sigma , brightness, size_category)
1689182296859:dff <- na.omit(dff)
1689182311319:# Select most relevant data for further analising
1689182311321:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182313610:dff <- na.omit(dff)
1689182330006:# Select most relevant data for further analising
1689182330008:dff <- df %>% select(H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182332679:dff <- na.omit(dff)
1689182360593:# Select most relevant data for further analising
1689182360595:dff <- df %>% select(albedo, H, H_divd, diameter_sigma, epoch, brightness, size_category)
1689182361941:dff <- na.omit(dff)
1689182370482:# Select most relevant data for further analising
1689182370484:dff <- df %>% select(albedo, H, diameter, diameter_sigma, epoch, brightness, size_category)
1689182372231:dff <- na.omit(dff)
1689182385885:# Select most relevant data for further analising
1689182385886:dff <- df %>% select(albedo, diameter, diameter_sigma, epoch, brightness, size_category)
1689182387389:dff <- na.omit(dff)
1689182401769:# Select most relevant data for further analising
1689182401771:dff <- df %>% select(albedo, H, H_divd, epoch, brightness, size_category)
1689182404038:dff <- na.omit(dff)
1689182559093:# Select most relevant data for further analising
1689182559095:df <- df %>% select(name, albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182592553:# Select most relevant data for further analising
1689182592554:df <- df %>% select(pds, albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182599549:# Select most relevant data for further analising
1689182599551:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182601718:df <- na.omit(df)
1689182738628:# Select most relevant data for further analising
1689182738630:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182748359:# Select most relevant data for further analising
1689182748360:df <- df %>% select(H)
1689182750954:df <- na.omit(df)
1689182763795:# Select most relevant data for further analising
1689182763795:df <- df %>% select(albedo)
1689182780891:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689182821937:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689182821938:df <- df %>%
1689182821939:mutate(brightness = case_when(
1689182821940:H < 15 ~ "Very Bright",
1689182821940:H >= 15 & H < 20 ~ "Moderately Bright",
1689182821941:TRUE ~ "Dim"
1689182821941:))
1689182823691:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689182823693:df <- df %>%
1689182823693:mutate(size_category = case_when(
1689182823694:diameter < 1 ~ "Small",
1689182823694:diameter >= 1 & diameter <= 10 ~ "Medium",
1689182823695:diameter > 10 ~ "Large",
1689182823695:TRUE ~ "Unknown"
1689182823696:))
1689182825975:# H devided by 5 (needed for formula)
1689182825976:df <- df %>%
1689182825977:mutate(H_divd = H / 5)
1689182830798:head(dff)
1689182839632:# Select most relevant data for further analising
1689182839633:dff <- df %>% select(albedo)
1689182842251:head(dff)
1689182844703:dff <- na.omit(dff)
1689182919709:head(df)
1689182977133:# Select most relevant data for further analising
1689182977133:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689182979097:dff$albedo[is.na(dff$albedo)] <- 0
1689182981730:dff <- na.omit(dff)
1689183000160:head(df)
1689183011478:row_number(df)
1689183017934:row_number(dff)
1689183303024:# Select most relevant data for further analising
1689183303026:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183306620:dff$albedo[is.na(dff$albedo)] <- 0
1689183319773:dff <- na.omit(dff)
1689183359441:dff$albedo[is.na(dff$albedo)] <- 0
1689183366574:head(dff)
1689183370308:head(dff)
1689183372827:head(dff)
1689183374514:head(dff)
1689183431670:# Select most relevant data for further analising
1689183431672:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183439914:dff$albedo[is.na(dff$albedo)] <- 0
1689183573432:dff <- na.replace(dff, 0)
1689183581497:dff <- na.omit(dff)
1689183594135:# Select most relevant data for further analising
1689183594137:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183596177:dff <- na.omit(dff)
1689183599715:# Select most relevant data for further analising
1689183599716:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183612683:dff$albedo <- na.replace(dff$albedo, 0)
1689183613785:dff <- na.omit(dff)
1689183630091:# Select most relevant data for further analising
1689183630093:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183635992:dff$albedo <- na.replace(dff$albedo, 0)
1689183648021:dff <- na.replace(dff, 0)
1689183650867:dff <- na.omit(dff)
1689183656176:head(dff)
1689183690123:# Select most relevant data for further analising
1689183690125:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183731240:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689183738590:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689183738592:df <- df %>%
1689183738592:mutate(brightness = case_when(
1689183738593:H < 15 ~ "Very Bright",
1689183738593:H >= 15 & H < 20 ~ "Moderately Bright",
1689183738594:TRUE ~ "Dim"
1689183738594:))
1689183741254:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689183741255:df <- df %>%
1689183741256:mutate(size_category = case_when(
1689183741256:diameter < 1 ~ "Small",
1689183741257:diameter >= 1 & diameter <= 10 ~ "Medium",
1689183741257:diameter > 10 ~ "Large",
1689183741258:TRUE ~ "Unknown"
1689183741258:))
1689183743843:# H devided by 5 (needed for formula)
1689183743845:df <- df %>%
1689183743845:mutate(H_divd = H / 5)
1689183745723:# Sanity columns
1689183745725:colnames(df)
1689183768615:# Select most relevant data for further analising
1689183768616:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689183777873:# Clean data
1689183777873:df <- na.replace(df, 0)
1689183779850:df <- na.omit(df)
1689183910336:# Show selected data
1689183910337:kable(head(df, n = 10L),
1689183910337:col.names = colnames(df),
1689183910338:caption = "Table view of filtered data",
1689183910338:format = "html",
1689183910339:align = "c"
1689183910339:)
1689183920999:df$albedo <- (df$albedo - min(df$albedo)) / (max(df$albedo) - min(df$albedo))
1689183941117:# Data split
1689183941119:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689183942826:# Check the split out
1689183942827:df_split
1689183957756:# Formula
1689183957758:formula <- albedo ~ ((1329 * 10 (-H_divd))/diameter)^2
1689183959567:# Regression
1689183959568:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689183965071:# Traing the model
1689183965073:m1 <- ml_fit(model1, df_split$training)
1689183973438:# End the session
1689183973440:spark_disconnect(sc)
1689183985575:# Connect
1689183985575:sc <- sparklyr::spark_connect(master = "local")
1689183994867:datasetPath <- "/data/dataset.csv"
1689183994917:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184003940:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689184003940:df <- df %>%
1689184003940:mutate(brightness = case_when(
1689184003940:H < 15 ~ "Very Bright",
1689184003940:H >= 15 & H < 20 ~ "Moderately Bright",
1689184003941:TRUE ~ "Dim"
1689184003941:))
1689184004136:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689184004136:df <- df %>%
1689184004137:mutate(size_category = case_when(
1689184004137:diameter < 1 ~ "Small",
1689184004138:diameter >= 1 & diameter <= 10 ~ "Medium",
1689184004138:diameter > 10 ~ "Large",
1689184004139:TRUE ~ "Unknown"
1689184004139:))
1689184004514:# H devided by 5 (needed for formula)
1689184004515:df <- df %>%
1689184004516:mutate(H_divd = H / 5)
1689184011430:# Sanity columns
1689184011431:colnames(df)
1689184020587:# Select most relevant data for further analising
1689184020589:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184028992:# Clean data
1689184028994:df$albedo <- na.replace(df$albedo, 0)
1689184073228:# Clean data
1689184073230:df$albedo <- replace(df$albedo, is.na(df$albedo), 0)
1689184077647:df <- na.omit(df)
1689184087996:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184103888:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689184103889:df <- df %>%
1689184103890:mutate(brightness = case_when(
1689184103890:H < 15 ~ "Very Bright",
1689184103891:H >= 15 & H < 20 ~ "Moderately Bright",
1689184103891:TRUE ~ "Dim"
1689184103892:))
1689184105842:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689184105843:df <- df %>%
1689184105843:mutate(size_category = case_when(
1689184105844:diameter < 1 ~ "Small",
1689184105845:diameter >= 1 & diameter <= 10 ~ "Medium",
1689184105845:diameter > 10 ~ "Large",
1689184105846:TRUE ~ "Unknown"
1689184105846:))
1689184108780:# H devided by 5 (needed for formula)
1689184108782:df <- df %>%
1689184108782:mutate(H_divd = H / 5)
1689184110386:# Sanity columns
1689184110388:colnames(df)
1689184113154:# Select most relevant data for further analising
1689184113155:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184118028:# Select most relevant data for further analising
1689184118028:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184127948:# Clean data
1689184127950:dff$albedo <- replace(dff$albedo, is.na(dff$albedo), 0)
1689184130872:dff <- na.omit(dff)
1689184134193:# Select most relevant data for further analising
1689184134195:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184136372:dff <- na.omit(dff)
1689184210683:# Clean data
1689184210685:dff$albedo <- replace(dff$albedo, is.na(dff$albedo), 0)
1689184212772:dff <- na.omit(dff)
1689184216545:# Select most relevant data for further analising
1689184216547:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184223260:# Clean data
1689184223262:dff$albedo <- replace(dff$albedo, is.na(dff$albedo), 0)
1689184229108:dff <- na.omit(dff)
1689184297802:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184305916:# Filter data
1689184305917:df <- df %>%
1689184305918:filter(!(is.na(diameter_sigma) ||
1689184305918:is.na(albedo) ||
1689184305919:is.na(H) ||
1689184305919:is.na(diameter)||
1689184305920:is.na(epoch) ||
1689184305920:is.na(name)
1689184305921:))
1689184311768:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689184311769:df <- df %>%
1689184311770:mutate(brightness = case_when(
1689184311770:H < 15 ~ "Very Bright",
1689184311771:H >= 15 & H < 20 ~ "Moderately Bright",
1689184311771:TRUE ~ "Dim"
1689184311772:))
1689184313276:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689184313278:df <- df %>%
1689184313278:mutate(size_category = case_when(
1689184313279:diameter < 1 ~ "Small",
1689184313279:diameter >= 1 & diameter <= 10 ~ "Medium",
1689184313280:diameter > 10 ~ "Large",
1689184313280:TRUE ~ "Unknown"
1689184313281:))
1689184314527:# H devided by 5 (needed for formula)
1689184314528:df <- df %>%
1689184314529:mutate(H_divd = H / 5)
1689184316154:# Sanity columns
1689184316155:colnames(df)
1689184318843:# Select most relevant data for further analising
1689184318845:dff <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689184321101:dff <- na.omit(dff)
1689184346344:View(dff)
1689184354771:View(dff)
1689184356968:View(dff)
1689184367305:dff[["albedo"]]
1689184371783:dff[["H"]]
1689184376261:dff[["albedo"]]\
1689184412342:df <- na.omit(df)
1689184428350:nrow(df)
1689184435990:nrow(dff)
1689184444126:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184467113:# Filter data
1689184467114:df <- df %>%
1689184467115:filter(!(is.na(diameter_sigma) ||
1689184467115:is.na(albedo) ||
1689184467116:is.na(H) ||
1689184467116:is.na(diameter)||
1689184467117:is.na(epoch) ||
1689184467117:is.na(name)
1689184467118:))
1689184474788:nrow(df)
1689184482250:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184492578:nrow(df)
1689184501982:print(nrow(df))
1689184520028:rows <- nrow(df)
1689184521434:print(rows)
1689184572343:count(df)
1689184577082:# Filter data
1689184577084:df <- df %>%
1689184577084:filter(!(is.na(diameter_sigma) ||
1689184577085:is.na(albedo) ||
1689184577085:is.na(H) ||
1689184577086:is.na(diameter)||
1689184577086:is.na(epoch) ||
1689184577087:is.na(name)
1689184577087:))
1689184587157:count(df)
1689184641542:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184647418:count(df)
1689184650199:# Filter data
1689184650201:df <- df %>%
1689184650201:filter(!(is.na(diameter_sigma) ||
1689184650202:is.na(albedo) ||
1689184650202:is.na(H) ||
1689184650203:is.na(diameter)||
1689184650203:is.na(epoch)
1689184650204:))
1689184652578:count(df)
1689184676473:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184688111:# Filter data
1689184688112:df <- df %>%
1689184688113:filter(!(is.na(diameter_sigma) ||
1689184688113:is.na(H) ||
1689184688114:is.na(diameter)||
1689184688114:is.na(epoch)
1689184688115:))
1689184696555:count(df)
1689184770233:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184777151:count(df)
1689184779370:# Filter data
1689184779372:df <- df %>%
1689184779373:filter(!(is.na(H) ||
1689184779373:is.na(diameter)||
1689184779374:is.na(epoch)
1689184779374:))
1689184781832:count(df)
1689184794871:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184805051:count(df)
1689184808116:# Filter data
1689184808117:df <- df %>%
1689184808118:filter(!(is.na(H) ||
1689184808119:is.na(diameter)
1689184808119:))
1689184810294:count(df)
1689184818405:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184824755:# Filter data
1689184824757:df <- df %>%
1689184824757:filter(!(is.na(H)
1689184824758:))
1689184826381:count(df)
1689184865251:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689184871836:# Filter data
1689184871838:df <- df %>%
1689184871838:filter(!(is.na(H) || is.na(epoch)))
1689184873677:count(df)
1689184909863:# Filter data
1689184909865:df <- df %>%
1689184909865:filter(!(is.na(H) || is.na(epoch) || is.na(diameter)))
1689184912975:count(df)
1689184924949:# Filter data
1689184924952:df <- df %>%
1689184924953:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma)))
1689184926102:count(df)
1689184932273:df <- df %>%
1689184932274:dplyr::mutate(albedo = sparklyr::na.fill(albedo, 0))
1689184948381:df <- df %>%
1689184948381:mutate(albedo = na.fill(albedo, 0))
1689184962186:df <- df %>%
1689184962187:mutate(albedo = na.replace(albedo, 0))
1689184990470:df <- df %>%
1689184990472:dplyr::mutate(albedo = sparklyr::na.fill(albedo, 0))
1689185068715:df <- df %>%
1689185068717:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689185079905:count(df)
1689185090930:df <- na.omit(df)
1689185108772:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689185119073:# Filter data
1689185119075:df <- df %>%
1689185119075:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma)))
1689185121093:count(df)
1689185132627:df <- df %>%
1689185132628:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689185138665:count(df)
1689185179705:head(df)
1689185190060:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689185190061:df <- df %>%
1689185190062:mutate(brightness = case_when(
1689185190062:H < 15 ~ "Very Bright",
1689185190063:H >= 15 & H < 20 ~ "Moderately Bright",
1689185190064:TRUE ~ "Dim"
1689185190064:))
1689185192153:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689185192155:df <- df %>%
1689185192155:mutate(size_category = case_when(
1689185192156:diameter < 1 ~ "Small",
1689185192156:diameter >= 1 & diameter <= 10 ~ "Medium",
1689185192157:diameter > 10 ~ "Large",
1689185192157:TRUE ~ "Unknown"
1689185192158:))
1689185233102:# H devided by 5 (needed for formula)
1689185233104:df <- df %>%
1689185233104:mutate(H_divd = H / 5)
1689185235836:# Sanity columns
1689185235837:colnames(df)
1689185244509:# Select most relevant data for further analising
1689185244511:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689185254215:count(df)
1689185266917:df <- na.omit(df)
1689185272543:count(df)
1689185282839:# Show selected data
1689185282841:kable(head(df, n = 10L),
1689185282841:col.names = colnames(df),
1689185282842:caption = "Table view of filtered data",
1689185282842:format = "html",
1689185282843:align = "c"
1689185282844:)
1689185291728:# Data split
1689185291729:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689185293188:# Check the split out
1689185293190:df_split
1689185301299:# Formula
1689185301301:formula <- albedo ~ ((1329 * 10 (-H_divd))/diameter)^2
1689185306265:# Regression
1689185306266:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689185347954:# Check the split out
1689185347955:df_split
1689185880825:datasetPath <- "/data/dataset.csv"
1689185881737:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689185926279:# Connect
1689185926280:sc <- sparklyr::spark_connect(master = "local")
1689185929639:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689185938385:# Filter data
1689185938386:df <- df %>%
1689185938387:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma))) %>%
1689185938388:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689185940769:# Check how much is lost
1689185940770:count(df)
1689185945776:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689185945778:df <- df %>%
1689185945778:mutate(brightness = case_when(
1689185945779:H < 20 ~ "Can't be seen",
1689185945779:H >= 15 & H < 20 ~ "Moderately Bright",
1689185945780:TRUE ~ "Can't be seen"
1689185945780:))
1689185948160:# Create a new column
1689185948162:df <- df %>%
1689185948162:mutate(can_be_seen = case_when(
1689185948163:H < 20 ~ 0,
1689185948163:H >= 20 ~ 1,
1689185948164:TRUE ~ 0
1689185948164:))
1689185951030:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689185951031:df <- df %>%
1689185951032:mutate(size_category = case_when(
1689185951032:diameter < 1 ~ "Small",
1689185951033:diameter >= 1 & diameter <= 10 ~ "Medium",
1689185951033:diameter > 10 ~ "Large",
1689185951034:TRUE ~ "Unknown"
1689185951034:))
1689185954111:# H devided by 5 (needed for formula)
1689185954113:df <- df %>%
1689185954113:mutate(H_divd = H / 5)
1689185956158:# Sanity columns
1689185956159:colnames(df)
1689185961896:# Select most relevant data for further analising
1689185961897:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689185964452:# Just in case
1689185964454:df <- na.omit(df)
1689185968288:# Show selected data
1689185968289:kable(head(df, n = 10L),
1689185968290:col.names = colnames(df),
1689185968290:caption = "Table view of filtered data",
1689185968291:format = "html",
1689185968291:align = "c"
1689185968292:)
1689186048800:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689186055213:# Filter data
1689186055215:df <- df %>%
1689186055215:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma))) %>%
1689186055216:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689186057040:# Check how much is lost
1689186057041:count(df)
1689186061315:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689186061317:df <- df %>%
1689186061317:mutate(brightness = case_when(
1689186061318:H < 20 ~ "Can't be seen",
1689186061318:H >= 20 ~ "Moderately Bright",
1689186061319:TRUE ~ "Can't be seen"
1689186061319:))
1689186063193:# Create a new column
1689186063195:df <- df %>%
1689186063195:mutate(can_be_seen = case_when(
1689186063196:H < 20 ~ 0,
1689186063196:H >= 20 ~ 1,
1689186063197:TRUE ~ 0
1689186063197:))
1689186065662:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689186065664:df <- df %>%
1689186065664:mutate(size_category = case_when(
1689186065665:diameter < 1 ~ "Small",
1689186065665:diameter >= 1 & diameter <= 10 ~ "Medium",
1689186065665:diameter > 10 ~ "Large",
1689186065666:TRUE ~ "Unknown"
1689186065666:))
1689186068673:# H devided by 5 (needed for formula)
1689186068674:df <- df %>%
1689186068675:mutate(H_divd = H / 5)
1689186069887:# Sanity columns
1689186069889:colnames(df)
1689186071777:# Select most relevant data for further analising
1689186071779:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689186074950:# Just in case
1689186074952:df <- na.omit(df)
1689186080149:# Show selected data
1689186080150:kable(head(df, n = 10L),
1689186080151:col.names = colnames(df),
1689186080152:caption = "Table view of filtered data",
1689186080152:format = "html",
1689186080153:align = "c"
1689186080153:)
1689186276024:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689186284994:# Filter data
1689186284995:df <- df %>%
1689186284996:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma))) %>%
1689186284997:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689186287299:# Check how much is lost
1689186287301:count(df)
1689186289133:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689186289134:df <- df %>%
1689186289134:mutate(brightness = case_when(
1689186289135:albedo < 0.25 ~ "Can't be seen",
1689186289136:albedo >= 0.25 ~ "Moderately Bright",
1689186289136:TRUE ~ "Can't be seen"
1689186289137:))
1689186295256:# Create a new column
1689186295258:df <- df %>%
1689186295258:mutate(can_be_seen = case_when(
1689186295259:albedo < 0.25 ~ 0,
1689186295259:albedo >= 0.25 ~ 1,
1689186295260:TRUE ~ 0
1689186295260:))
1689186297177:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689186297179:df <- df %>%
1689186297180:mutate(size_category = case_when(
1689186297180:diameter < 1 ~ "Small",
1689186297181:diameter >= 1 & diameter <= 10 ~ "Medium",
1689186297181:diameter > 10 ~ "Large",
1689186297182:TRUE ~ "Unknown"
1689186297182:))
1689186299372:# H devided by 5 (needed for formula)
1689186299374:df <- df %>%
1689186299374:mutate(H_divd = H / 5)
1689186300707:# Sanity columns
1689186300709:colnames(df)
1689186303972:# Select most relevant data for further analising
1689186303973:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, brightness, size_category)
1689186306355:# Just in case
1689186306357:df <- na.omit(df)
1689186310452:# Show selected data
1689186310454:kable(head(df, n = 10L),
1689186310455:col.names = colnames(df),
1689186310455:caption = "Table view of filtered data",
1689186310456:format = "html",
1689186310456:align = "c"
1689186310457:)
1689186371100:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689186377561:# Filter data
1689186377562:df <- df %>%
1689186377563:filter(!(is.na(H) || is.na(epoch) || is.na(diameter) || is.na(diameter_sigma))) %>%
1689186377563:mutate(albedo = ifelse(is.na(albedo), 0, albedo))
1689186379225:# Check how much is lost
1689186379226:count(df)
1689186380710:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689186380712:df <- df %>%
1689186380712:mutate(brightness = case_when(
1689186380713:albedo < 0.25 ~ "Can't be seen",
1689186380713:albedo >= 0.25 ~ "Moderately Bright",
1689186380714:TRUE ~ "Can't be seen"
1689186380714:))
1689186383038:# Create a new column
1689186383040:df <- df %>%
1689186383040:mutate(can_be_seen = case_when(
1689186383041:albedo < 0.25 ~ 0,
1689186383041:albedo >= 0.25 ~ 1,
1689186383042:TRUE ~ 0
1689186383042:))
1689186385727:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689186385728:df <- df %>%
1689186385729:mutate(size_category = case_when(
1689186385730:diameter < 1 ~ "Small",
1689186385730:diameter >= 1 & diameter <= 10 ~ "Medium",
1689186385731:diameter > 10 ~ "Large",
1689186385731:TRUE ~ "Unknown"
1689186385732:))
1689186387515:# H devided by 5 (needed for formula)
1689186387517:df <- df %>%
1689186387518:mutate(H_divd = H / 5)
1689186388787:# Sanity columns
1689186388788:colnames(df)
1689186391026:# Select most relevant data for further analising
1689186391026:df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, can_be_seen, brightness, size_category)
1689186393906:# Just in case
1689186393907:df <- na.omit(df)
1689186396787:# Show selected data
1689186396789:kable(head(df, n = 10L),
1689186396789:col.names = colnames(df),
1689186396790:caption = "Table view of filtered data",
1689186396790:format = "html",
1689186396790:align = "c"
1689186396791:)
1689186444423:# Data split
1689186444425:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689186446377:# Check the split out
1689186446378:df_split
1689186452846:# Formula
1689186452847:formula <- albedo ~ ((1329 * 10 (-H_divd))/diameter)^2
1689186456255:# Regression
1689186456257:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689186643866:# Formula
1689186643867:formula <- can_be_seen ~ H + H_divd + diameter + albedo
1689186654554:# Regression
1689186654555:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689186661270:# Traing the model
1689186661270:m1 <- ml_fit(model1, df_split$training)
1689186680620:View(model1)
1689187104796:# Regression
1689187104797:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689187175784:# Evaluate the model
1689187175785:result <- ml_evaluate(model1, data_split$test)
1689187184908:# Evaluate the model
1689187184910:result <- ml_evaluate(model1, df_split$test)
1689187192567:result
1689187207824:ml_summart(result)
1689187213012:ml_summary(result)
1689187221164:result
1689187238457:result$weighted_f_measure()
1689187251541:result$accuracy()
1689187412100:# Regression
1689187412101:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 1, family = "binomial")
1689187416159:# Evaluate the model
1689187416160:result <- ml_evaluate(model1, df_split$test)
1689187418384:result$accuracy()
1689187444294:View(result)
1689187496010:result$weighted_precision()
1689187500099:# Regression
1689187500100:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689187502156:# Evaluate the model
1689187502157:result <- ml_evaluate(model1, df_split$test)
1689187506561:result$weighted_precision()
1689187547757:# Formula
1689187547758:formula <- can_be_seen ~ H + H_divd + diameter
1689187550422:# Regression
1689187550423:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689187553009:# Evaluate the model
1689187553010:result <- ml_evaluate(model1, df_split$test)
1689187554661:result$weighted_precision()
1689187573496:# Regression
1689187573497:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 1, family = "binomial")
1689187576161:# Evaluate the model
1689187576163:result <- ml_evaluate(model1, df_split$test)
1689187577722:result$weighted_precision()
1689187584486:# Formula
1689187584487:formula <- can_be_seen ~ H + H_divd + diameter
1689187586309:# Regression
1689187586311:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 1, family = "binomial")
1689187588366:# Evaluate the model
1689187588367:result <- ml_evaluate(model1, df_split$test)
1689187590492:result$weighted_precision()
1689187597994:+ albedo
1689187600732:# Formula
1689187600732:formula <- can_be_seen ~ H + H_divd + diameter + albedo
1689187602903:# Regression
1689187602905:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 5, family = "binomial")
1689187604975:# Evaluate the model
1689187604976:result <- ml_evaluate(model1, df_split$test)
1689187605907:result$weighted_precision()
1689187609540:# Regression
1689187609542:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 1, family = "binomial")
1689187611913:# Evaluate the model
1689187611915:result <- ml_evaluate(model1, df_split$test)
1689187614105:result$weighted_precision()
1689187620296:# Regression
1689187620297:model1 <- ml_logistic_regression(df_split$training, formula, max_iter = 3, family = "binomial")
1689187622409:# Evaluate the model
1689187622409:result <- ml_evaluate(model1, df_split$test)
1689187625701:result$weighted_precision()
1689188389989:#
1689188389990:max_iterations <- 5
1689188391078:results <- c()
1689188457256:for(i in max_iterations){
1689188457257:# Regression
1689188457257:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188457258:# Evaluate the model
1689188457259:result <- ml_evaluate(model, df_split$test)
1689188457260:# Save results
1689188457260:result[i] <- result
1689188457260:}
1689188531642:for(i in max_iterations){
1689188531643:# Regression
1689188531643:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188531644:# Evaluate the model
1689188531645:result <- ml_evaluate(model, df_split$test)
1689188531646:# Save results
1689188531646:results[i] <- result
1689188531647:}
1689188628977:results <- vector(,max_iterations)
1689188631755:for(i in max_iterations){
1689188631757:# Regression
1689188631757:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188631758:# Evaluate the model
1689188631759:result <- ml_evaluate(model, df_split$test)
1689188631760:# Save results
1689188631760:results[i] <- result
1689188631761:}
1689188673449:results <- c()
1689188676206:for(i in max_iterations){
1689188676206:# Regression
1689188676207:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188676207:# Evaluate the model
1689188676207:result <- ml_evaluate(model, df_split$test)
1689188676208:# Save results
1689188676208:results[i] <- result
1689188676208:}
1689188750191:# Regression
1689188750193:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188755522:# Evaluate the model
1689188755523:result <- ml_evaluate(model, df_split$test)
1689188759122:# Save results
1689188759123:results[i] <- result
1689188776658:result$accuracy()
1689188792932:for(i in max_iterations){
1689188792933:# Regression
1689188792934:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689188792935:# Evaluate the model
1689188792935:result <- ml_evaluate(model, df_split$test)
1689188792936:# Save results
1689188792937:results[i] <- result
1689188792937:}
1689188797379:results[i]$accuracy
1689189090119:for(i in max_iterations){
1689189090121:i
1689189090121:}
1689189097757:for(i in max_iterations){
1689189097758:print(i)
1689189097759:}
1689189144593:for(i in 1:max_iterations){
1689189144595:print(i)
1689189144595:}
1689189179874:for(i in 1:max_iterations){
1689189179875:# Regression
1689189179876:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689189179877:# Evaluate the model
1689189179877:result <- ml_evaluate(model, df_split$test)
1689189179878:# Save results
1689189179879:results[i] <- result
1689189179879:}
1689189206420:results[i]$accuracy()
1689189378227:for (i in 1:max_iterations) {
1689189378228:# Create logistic regression estimator
1689189378229:estimator <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689189378230:# Train the model
1689189378230:model <- ml_fit(estimator, df_split$training)
1689189378231:# Evaluate the model
1689189378232:result <- ml_evaluate(model, df_split$test)
1689189378233:# Save results
1689189378233:results[[i]] <- result
1689189378234:}
1689189406352:# Traing the model
1689189406352:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689189411010:for (i in 1:max_iterations) {
1689189411012:# Traing the model
1689189411012:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689189411013:# Evaluate the model
1689189411014:result <- ml_evaluate(model, df_split$test)
1689189411015:# Save results
1689189411016:results[[i]] <- result
1689189411016:}
1689189436499:results[[1]]$accuracy()
1689189446039:results[[1]]$weighted_precision()
1689189479575:results[[1]]$weighted_recall()
1689191001487:# Plottin'
1689191001488:df <- data.frame(
1689191001489:i = 1:max_iterations,
1689191001489:wp = sapply(results, function(x) x$weighted.precision),
1689191001490:wr = sapply(results, function(x) x$weighted.recall),
1689191001490:wf = sapply(results, function(x) x$weighted.f.measure),
1689191001491:aur = sapply(results, function(x) x$area.under.roc),
1689191001492:a = sapply(results, function(x) x$accuracy)
1689191001492:)
1689191488623:# Plottin'
1689191488625:result_data <- data.frame(
1689191488625:i=max_iterations,
1689191488626:wp=results[[1]]$weighted_precision(),
1689191488626:wr=results[[1]]$weighted_recall(),
1689191488627:a=results[[1]]$accuracy())
1689191491254:plot1 <- result_data %>%
1689191491256:ggplot(aes(i, wp, color=wp)) +
1689191491256:geom_line(size=2) +
1689191491257:scale_x_continuous(breaks=max_iterations) +
1689191491257:scale_y_continuous(breaks=results[[1]]$weighted_precision()) +
1689191491258:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689191491258:theme(text = element_text(size = 16)) +
1689191491259:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689191510187:plot1 <- result_data %>%
1689191510188:ggplot(aes(i, wp, color=wp)) +
1689191510189:geom_line(linewidth=2) +
1689191510190:scale_x_continuous(breaks=max_iterations) +
1689191510190:scale_y_continuous(breaks=results[[1]]$weighted_precision()) +
1689191510191:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689191510191:theme(text = element_text(size = 16)) +
1689191510192:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689191529951:plot1
1689191872787:View(results)
1689191920226:# Plottin'
1689191920228:result_data <- data.frame(
1689191920228:i=max_iterations,
1689191920229:wp=results[[1:5]]$weighted_precision(),
1689191920229:wr=results[[1:5]]$weighted_recall(),
1689191920230:a=results[[1:5]]$accuracy())
1689191930217:plot1 <- result_data %>%
1689191930218:ggplot(aes(i, wp, color=wp)) +
1689191930219:geom_line(linewidth=2) +
1689191930219:scale_x_continuous(breaks=max_iterations) +
1689191930220:scale_y_continuous(breaks=results[[1:5]]$weighted_precision()) +
1689191930220:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689191930221:theme(text = element_text(size = 16)) +
1689191930222:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689192209125:results[[1]]$weighted_recall()
1689192236609:result_data <- do.call(rbind, results)
1689192242775:result_data
1689192338267:results[[1]]$weighted_recall()
1689192371625:# Plottin'
1689192371627:result_data <- data.frame(
1689192371627:i=max_iterations,
1689192371628:wp=results[[i]]$weighted_precision(),
1689192371628:wr=results[[i]]$weighted_recall(),
1689192371628:a=results[[i]]$accuracy())
1689192377767:result_data
1689192401032:# Plottin'
1689192401034:result_data <- data.frame(
1689192401034:i= 1:max_iterations,
1689192401035:wp=results[[i]]$weighted_precision(),
1689192401035:wr=results[[i]]$weighted_recall(),
1689192401036:a=results[[i]]$accuracy())
1689192407321:result_data
1689192437011:plot1 <- result_data %>%
1689192437012:ggplot(aes(i, wp, color=wp)) +
1689192437012:geom_line(linewidth=2) +
1689192437013:scale_x_continuous(breaks=max_iterations) +
1689192437013:scale_y_continuous(breaks=results[[i]]$weighted_precision()) +
1689192437013:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689192437013:theme(text = element_text(size = 16)) +
1689192437013:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689192439508:plot1
1689192506560:# Plottin'
1689192506561:result_data <- data.frame(
1689192506562:i= c(1:max_iterations),
1689192506562:wp=results[[i]]$weighted_precision(),
1689192506562:wr=results[[i]]$weighted_recall(),
1689192506563:a=results[[i]]$accuracy())
1689192511171:plot1 <- result_data %>%
1689192511173:ggplot(aes(i, wp, color=wp)) +
1689192511173:geom_line(linewidth=2) +
1689192511174:scale_x_continuous(breaks=max_iterations) +
1689192511174:scale_y_continuous(breaks=results[[i]]$weighted_precision()) +
1689192511174:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689192511175:theme(text = element_text(size = 16)) +
1689192511175:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689192512186:plot1
1689192525999:result_data
1689192590497:results <- data.frame(
1689192590498:i=c(1:max_iterations),
1689192590499:wp=c(),
1689192590499:wr=c(),
1689192590500:a=c()
1689192590500:)
1689192610793:results <- data.frame(
1689192610795:i=c(1:max_iterations),
1689192610795:wp=c(,5),
1689192610796:wr=c(,5),
1689192610796:a=c(,5)
1689192610796:)
1689192636453:results <- data.frame(
1689192636455:i=c(),
1689192636455:wp=c(),
1689192636456:wr=c(),
1689192636456:a=c()
1689192636456:)
1689192776990:for (i in 1:max_iterations) {
1689192776992:# Traing the model
1689192776992:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689192776992:# Evaluate the model
1689192776992:result <- ml_evaluate(model, df_split$test)
1689192776993:# Save results
1689192776993:i[i] <- i
1689192776993:results$wp[i] <- result$weighted_precision()
1689192776994:results$wr[i] <- result$weighted_recall()
1689192776994:results$a[i] <- result$accuracy()
1689192776994:}
1689192869321:for (i in 1:max_iterations) {
1689192869323:# Traing the model
1689192869323:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689192869323:# Evaluate the model
1689192869324:result <- ml_evaluate(model, df_split$test)
1689192869324:# Save results
1689192869324:i[i] <- i
1689192869325:results$wp[i] <- result$weighted_precision()
1689192869325:results$wr[i] <- result$weighted_recall()
1689192869325:results$a[i] <- result$accuracy()
1689192869326:}
1689192891586:# Plottin'
1689192891588:result_data <- data.frame(
1689192891588:i= 1:max_iterations,
1689192891588:wp=results[[i]]$weighted_precision(),
1689192891589:wr=results[[i]]$weighted_recall(),
1689192891589:a=results[[i]]$accuracy()
1689192891590:)
1689192910199:View(result)
1689192917054:View(results)
1689192927910:results[[1]]$weighted_recall()
1689192931040:for (i in 1:max_iterations) {
1689192931041:# Traing the model
1689192931041:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689192931042:# Evaluate the model
1689192931042:result <- ml_evaluate(model, df_split$test)
1689192931043:# Save results
1689192931043:results[[i]] <- result
1689192931043:}
1689192952033:for (i in 1:max_iterations) {
1689192952035:# Traing the model
1689192952035:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689192952036:# Evaluate the model
1689192952036:result <- ml_evaluate(model, df_split$test)
1689192952037:# Save results
1689192952037:results[[i]] <- result
1689192952038:}
1689192959488:# Logistic regression with different max iterations
1689192959490:max_iterations <- 5
1689192960378:results <- vector("list", max_iterations)
1689192961696:for (i in 1:max_iterations) {
1689192961698:# Traing the model
1689192961698:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689192961699:# Evaluate the model
1689192961699:result <- ml_evaluate(model, df_split$test)
1689192961700:# Save results
1689192961700:results[[i]] <- result
1689192961701:}
1689192968931:results[[1]]$weighted_recall()
1689192974383:results[[5]]$weighted_recall()
1689192998885:View(results)
1689193176136:# Plottin'
1689193176138:result_data <- data.frame(
1689193176138:i= c(1:max_iterations),
1689193176139:wp=c(results[[1]]$weighted_precision():results[[max_iterations]]$weighted_precision())
1689193176139:)
1689193179848:View(result_data)
1689193460428:for (i in 1:max_iterations) {
1689193460430:# Traing the model
1689193460430:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689193460430:# Evaluate the model
1689193460431:result <- ml_evaluate(model, df_split$test)
1689193460432:# Save results
1689193460432:results$wp[i] <- result$weighted_precision()
1689193460433:results$wr[i] <- result$weighted_recall()
1689193460433:results$a[i] <- result$accuracy()
1689193460434:}
1689193470290:results
1689193512390:plot1 <- results %>%
1689193512390:ggplot(aes(i, wp, color=wp)) +
1689193512391:geom_line(linewidth=2) +
1689193512391:scale_x_continuous(breaks=max_iterations) +
1689193512392:scale_y_continuous(breaks=results$wp) +
1689193512392:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689193512392:theme(text = element_text(size = 16)) +
1689193512393:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689193595528:result_data <- do.call(rbind, results)
1689193601957:# Plottin'
1689193601959:plot1 <- result_data %>%
1689193601959:ggplot(aes(i, wp, color=wp)) +
1689193601959:geom_line(linewidth=2) +
1689193601960:scale_x_continuous(breaks=max_iterations) +
1689193601960:scale_y_continuous(breaks=results$wp) +
1689193601961:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689193601961:theme(text = element_text(size = 16)) +
1689193601962:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689193623330:# Sanity
1689193623332:results
1689193686261:# Plottin'
1689193686262:plot1 <- results %>%
1689193686263:ggplot(aes(i, wp, color=wp)) +
1689193686263:geom_line(linewidth=2) +
1689193686263:scale_x_continuous(breaks=1:max_iterations) +
1689193686263:scale_y_continuous(breaks=results$wp) +
1689193686264:scale_color_gradient(low = "#FF2266", high="#6622FF") +
1689193686264:theme(text = element_text(size = 16)) +
1689193686264:labs(x="Num of iterations", y="Precision", title = "a) Precision dependency on number of iterations")
1689193698758:rlang::last_trace()
1689193971435:for (i in 1:max_iterations) {
1689193971437:# Traing the model
1689193971437:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689193971438:# Evaluate the model
1689193971438:result <- ml_evaluate(model, df_split$test)
1689193971439:# Save results
1689193971439:result_data$wp[i] <- result$weighted_precision()
1689193971440:result_data$wr[i] <- result$weighted_recall()
1689193971440:result_data$a[i] <- result$accuracy()
1689193971441:}
1689193985090:# Logistic regression with different max iterations
1689193985091:max_iterations <- 5
1689193987462:# Create empty data frame
1689193987464:result_data <- data.frame(i = 1:max_iterations, wp = numeric(max_iterations), wr = numeric(max_iterations), a = numeric(max_iterations))
1689193990083:for (i in 1:max_iterations) {
1689193990085:# Traing the model
1689193990085:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689193990086:# Evaluate the model
1689193990087:result <- ml_evaluate(model, df_split$test)
1689193990087:# Save results
1689193990088:result_data$wp[i] <- result$weighted_precision()
1689193990088:result_data$wr[i] <- result$weighted_recall()
1689193990089:result_data$a[i] <- result$accuracy()
1689193990089:}
1689194003359:# Sanity
1689194003361:result_data
1689194009022:# Plotting
1689194009023:plot1 <- result_data %>%
1689194009024:ggplot(aes(i, wp, color = wp)) +
1689194009024:geom_line(linewidth = 2) +
1689194009024:scale_x_continuous(breaks = 1:max_iterations) +
1689194009025:scale_y_continuous(breaks = result_data$wp) +
1689194009025:scale_color_gradient(low = "#FF2266", high = "#6622FF") +
1689194009026:theme(text = element_text(size = 16)) +
1689194009026:labs(x = "Num of iterations", y = "Precision", title = "a) Precision dependency on number of iterations")
1689194011017:plot1
1689194087750:plot2 <- result_data %>%
1689194087750:ggplot(aes(i, wp, color = wp)) +
1689194087751:geom_line(linewidth = 2) +
1689194087751:scale_x_continuous(breaks = 1:max_iterations) +
1689194087752:scale_y_continuous(breaks = result_data$wr) +
1689194087752:scale_color_gradient(low = "#FF2266", high = "#6622FF") +
1689194087752:theme(text = element_text(size = 16)) +
1689194087753:labs(x = "Num of iterations", y = "Recall", title = "a) Recall dependency on number of iterations")
1689194089965:plot3 <- result_data %>%
1689194089966:ggplot(aes(i, wp, color = wp)) +
1689194089966:geom_line(linewidth = 2) +
1689194089967:scale_x_continuous(breaks = 1:max_iterations) +
1689194089967:scale_y_continuous(breaks = result_data$a) +
1689194089967:scale_color_gradient(low = "#FF2266", high = "#6622FF") +
1689194089968:theme(text = element_text(size = 16)) +
1689194089968:labs(x = "Num of iterations", y = "Accuracy", title = "a) Accuracy dependency on number of iterations")
1689194099287:plot2
1689194101358:plot3
1689194108365:plot1
1689194111451:plot2
1689194115049:plot3
1689194117232:plot1
1689194204370:for (i in 1:max_iterations) {
1689194204371:# Traing the model
1689194204372:model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
1689194204372:# Evaluate the model
1689194204372:result <- ml_evaluate(model, df_split$test)
1689194204373:# Save results
1689194204373:result_data$wp[i] <- result$weighted_precision()
1689194204373:result_data$wr[i] <- result$weighted_recall()
1689194204373:result_data$a[i] <- result$accuracy()
1689194204373:result_data$roc[i] <- result$area_under_roc()
1689194204374:}
1689194224348:# Sanity
1689194224350:result_data
1689194582583:# Plotting Accuracy
1689194582584:plot3 <- result_data %>%
1689194582584:ggplot(aes(i, wp, color = wp)) +
1689194582584:geom_line(linewidth = 2) +
1689194582584:scale_x_continuous(breaks = 1:max_iterations) +
1689194582585:scale_y_continuous(breaks = result_data$roc) +
1689194582585:scale_color_gradient(low = "#FF2266", high = "#6622FF") +
1689194582585:theme(text = element_text(size = 16)) +
1689194582585:labs(x = "Num of iterations", y = "ROC", title = "a) Accuracy dependency on number of iterations")
1689194586116:plot3
1689194589616:plot3
1689194594601:# Plotting Accuracy
1689194594602:plot3 <- result_data %>%
1689194594603:ggplot(aes(i, wp, color = wp)) +
1689194594603:geom_line(linewidth = 2) +
1689194594603:scale_x_continuous(breaks = 1:max_iterations) +
1689194594603:scale_y_continuous(breaks = result_data$a) +
1689194594604:scale_color_gradient(low = "#FF2266", high = "#6622FF") +
1689194594604:theme(text = element_text(size = 16)) +
1689194594604:labs(x = "Num of iterations", y = "Accuracy", title = "a) Accuracy dependency on number of iterations")
1689194597692:plot3
1689194706055:# Check the split out
1689194706056:df_split
1689194759695:bayes_model <- df_split$training %>% ml_naive_bayes(formula)
1689194875811:# Linear SVC classification
1689194875813:linear_svc_model <- df_split$training %>%
1689194875813:ml_linear_svc(formula)
1689194949362:# Decision 3 classification
1689194949363:decision_tree_classifier <- df_split$training %>%
1689194949364:ml_decision_tree_classifier(formula)
1689195106699:# Sanity
1689195106699:result_data
1689195260714:# Testing F1 Score for different methods
1689195260716:bayes_f1 <- ml_evaluate(bayes_model, df_split$test)
1689195265446:bayes_f1
1689195293750:bayes_f1 <- bayes_f1$Accuracy
1689195300277:bayes_f1
1689195315276:# Testing F1 Score for different methods
1689195315277:bayes_f1 <- ml_evaluate(bayes_model, df_split$test)$Accuracy
1689195402161:svc_accuracy <- ml_evaluate(liner_svc_model, df_split$test)$Accuracy
1689195424109:# Testing F1 Score for different methods
1689195424110:bayes_accuracy <- ml_evaluate(bayes_model, df_split$test)$Accuracy
1689195425751:svc_accuracy <- ml_evaluate(linear_svc_model, df_split$test)$Accuracy
1689195427484:d3_accuracy <- ml_evaluate(decision_tree_classifier, df_split$test)$Accuracy
1689195697156:View(df)
1689195966103:# Testing Accuracy Score for different methods using 4-cross validation
1689195966105:k4 <- function(dataset, model, formula){
1689195966105:dataset <- dataset %>%
1689195966105:sdf_random_split(seed=1,
1689195966106:s1=0.25,
1689195966106:s2=0.25,
1689195966107:s3=0.25,
1689195966107:s4=0.25)
1689195966107:training <- list(
1689195966108:s1 = sdf_bind_rows(dataset$s2, dataset$s3, dataset$s4),
1689195966109:s2 = sdf_bind_rows(dataset$s1, dataset$s3, dataset$s4),
1689195966109:s3 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s4),
1689195966109:s4 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s3)
1689195966110:)
1689195966111:trained = list(s1=model(training$s1, formula),
1689195966111:s2=model(training$s2, formula),
1689195966112:s3=model(training$s3, formula),
1689195966112:s4=model(training$s4, formula)
1689195966113:)
1689195966114:accuracy <- (ml_evaluate(trained$s1, dataset$s1)$Accuracy +
1689195966114:ml_evaluate(trained$s2, dataset$s2)$Accuracy +
1689195966115:ml_evaluate(trained$s3, dataset$s3)$Accuracy +
1689195966116:ml_evaluate(trained$s4, dataset$s4)$Accuracy
1689195966116:) / 4
1689195966117:}
1689196085565:bayes_k4_accuracy <- k4c(df, ml_naive_bayes, formula)
1689196091517:# Testing Accuracy Score for different methods using 4-cross validation
1689196091518:k4c <- function(dataset, model, formula){
1689196091519:dataset <- dataset %>%
1689196091519:sdf_random_split(seed=1,
1689196091520:s1=0.25,
1689196091520:s2=0.25,
1689196091520:s3=0.25,
1689196091521:s4=0.25)
1689196091521:training <- list(
1689196091522:s1 = sdf_bind_rows(dataset$s2, dataset$s3, dataset$s4),
1689196091522:s2 = sdf_bind_rows(dataset$s1, dataset$s3, dataset$s4),
1689196091523:s3 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s4),
1689196091523:s4 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s3)
1689196091523:)
1689196091524:trained = list(s1=model(training$s1, formula),
1689196091525:s2=model(training$s2, formula),
1689196091525:s3=model(training$s3, formula),
1689196091525:s4=model(training$s4, formula)
1689196091526:)
1689196091526:accuracy <- (ml_evaluate(trained$s1, dataset$s1)$Accuracy +
1689196091526:ml_evaluate(trained$s2, dataset$s2)$Accuracy +
1689196091527:ml_evaluate(trained$s3, dataset$s3)$Accuracy +
1689196091527:ml_evaluate(trained$s4, dataset$s4)$Accuracy
1689196091527:) / 4
1689196091527:}
1689196094014:bayes_k4_accuracy <- k4c(df, ml_naive_bayes, formula)
1689196122464:svc_k4_accuracy <- k4c(df, ml_linear_svc, formula)
1689196435019:d3_k4_accuracy <- k4c(df, ml_decision_tree_classifier, formula)
1689196504534:# Show selected data
1689196504536:kable(head(df, n = 10L),
1689196504536:col.names = colnames(df),
1689196504536:caption = "Table view of filtered data",
1689196504536:format = "html",
1689196504537:align = "c"
1689196504537:)
1689196657345:# Table View
1689196657346:knitr::kable(array(c("Bayes", "SVC", "Decision Tree",
1689196657346:bayes_accuracy, svc_accuracy, d3_accuracy,
1689196657347:bayes_k4_accuracy, svc_k4_accuracy, d3_accuracy),
1689196657347:dim = c(3, 3)),
1689196657347:col.names = c("Models", "Accuracy", "4-Cross Accuracy"),
1689196657347:label = "Comparing accuracy of different models",
1689196657348:align = "ccc",
1689196657348:format = "html"
1689196657348:)
1689197237088:# Clusterization
1689197237089:df_clusterization <- df %>%
1689197237089:sselect(size_category, diameter, diameter_sigma, H)
1689197244490:# Clusterization
1689197244492:df_clusterization <- df %>%
1689197244492:select(size_category, diameter, diameter_sigma, H)
1689197475347:# Clusterization
1689197475349:df_clusterization <- df %>%
1689197475349:select(size_category, diameter, diameter_sigma, H)
1689197633093:cluster_formula <- size_category ~ diameter + diameter_sigma + H
1689197667505:model_5 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 5)
1689197784174:cp1 <- model_5$centers %>%
1689197784174:ggplot(aes(diameter, H, color=H)) +
1689197784174:geom_point(size=5) +
1689197784175:scale_color_gradientn(colors = rainbow(10)) +
1689197784175:theme(text = element_text(size=16)) +
1689197784175:labs(x="quantified", y="number of samples", title = "a) K=5-means")
1689197789908:cp1
1689198225009:# Clusterizatio dataset
1689198225011:df_clusterization <- df %>%
1689198225011:select(size_category, diameter, diameter_sigma)
1689198226664:cluster_formula <- size_category ~ diameter + diameter_sigma
1689198229161:model_5 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 5)
1689198235408:cp1 <- model_5$centers %>%
1689198235409:ggplot(aes(diameter, diameter_sigma, color=diameter_sigma)) +
1689198235410:geom_point(size=5) +
1689198235410:scale_color_gradientn(colors = rainbow(10)) +
1689198235410:theme(text = element_text(size=16)) +
1689198235410:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689198237362:cp1
1689198930106:# Clusterizatio dataset
1689198930108:df_clusterization <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689198940908:# Filter clusterization data
1689198940909:df_clusterization <- df_clusterization %>%
1689198940909:filter(!(is.na(diameter) || is.na(e)))
1689198956699:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689198956701:df_clusterization <- df_clusterization %>%
1689198956701:mutate(size_category = case_when(
1689198956701:diameter < 1 ~ "Small",
1689198956702:diameter >= 1 & diameter < 5 ~ "Medium",
1689198956702:diameter >= 5 & diameter < 10 ~ "Large",
1689198956702:diameter >= 10 ~ "X-Large",
1689198956702:TRUE ~ "Unknown"
1689198956702:))
1689198998629:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689198998630:df_clusterization <- df_clusterization %>%
1689198998630:mutate(size_category = case_when(
1689198998631:diameter < 1 ~ "Small",
1689198998631:diameter >= 1 & diameter < 5 ~ "Medium",
1689198998631:diameter >= 5 & diameter < 10 ~ "Large",
1689198998631:diameter >= 10 ~ "X-Large",
1689198998632:TRUE ~ "Unknown"
1689198998632:))
1689199000406:df_clusterization <- df_clusterization %>% select(diameter, e, size_category)
1689199010188:View(df_clusterization)
1689199040333:count(df_clusterization)
1689199078539:cp1 <- model_5$centers %>%
1689199078541:ggplot(aes(diameter, e, color=e)) +
1689199078541:geom_point(size=5) +
1689199078541:scale_color_gradientn(colors = rainbow(10)) +
1689199078541:theme(text = element_text(size=16)) +
1689199078542:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199080608:cp1
1689199106924:# Clusterizatio dataset
1689199106925:df_clusterization <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689199115722:# Filter clusterization data
1689199115724:df_clusterization <- df_clusterization %>%
1689199115724:filter(!(is.na(diameter) || is.na(e)))
1689199120031:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689199120031:df_clusterization <- df_clusterization %>%
1689199120031:mutate(size_category = case_when(
1689199120032:diameter < 1 ~ "Small",
1689199120032:diameter >= 1 & diameter < 5 ~ "Medium",
1689199120032:diameter >= 5 & diameter < 10 ~ "Large",
1689199120033:diameter >= 10 ~ "X-Large",
1689199120033:TRUE ~ "Unknown"
1689199120033:))
1689199126937:df_clusterization <- df_clusterization %>% select(diameter, e, size_category)
1689199131953:head(df_clusterization)
1689199144667:cluster_formula <- size_category ~ diameter + e
1689199149737:model_5 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 5)
1689199163068:View(model_5)
1689199170321:cp1 <- model_5$centers %>%
1689199170322:ggplot(aes(diameter, e, color=e)) +
1689199170322:geom_point(size=5) +
1689199170323:scale_color_gradientn(colors = rainbow(10)) +
1689199170323:theme(text = element_text(size=16)) +
1689199170324:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199172127:cp1
1689199265336:model_10 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 10)
1689199279875:cp2 <- model_5$centers %>%
1689199279877:ggplot(aes(diameter, e, color=e)) +
1689199279877:geom_point(size=5) +
1689199279877:scale_color_gradientn(colors = rainbow(10)) +
1689199279878:theme(text = element_text(size=16)) +
1689199279878:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199281492:cp2
1689199313274:model_5 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 25)
1689199317447:cp1 <- model_5$centers %>%
1689199317449:ggplot(aes(diameter, e, color=e)) +
1689199317450:geom_point(size=5) +
1689199317450:scale_color_gradientn(colors = rainbow(10)) +
1689199317450:theme(text = element_text(size=16)) +
1689199317451:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199319402:cp1
1689199358627:cp2 <- model_10$centers %>%
1689199358629:ggplot(aes(diameter, e, color=e)) +
1689199358629:geom_point(size=5) +
1689199358630:scale_color_gradientn(colors = rainbow(10)) +
1689199358630:theme(text = element_text(size=16)) +
1689199358630:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199360759:cp2
1689199389143:model_20 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 20)
1689199425585:cplot_20 <- model_10$centers %>%
1689199425587:ggplot(aes(diameter, e, color=e)) +
1689199425587:geom_point(size=5) +
1689199425587:scale_color_gradientn(colors = rainbow(10)) +
1689199425588:theme(text = element_text(size=16)) +
1689199425588:labs(x="diameter", y="number of samples", title = "a) K=10-means")
1689199427021:cplot_20
1689199443067:cplot_20 <- model_20$centers %>%
1689199443068:ggplot(aes(diameter, e, color=e)) +
1689199443069:geom_point(size=5) +
1689199443069:scale_color_gradientn(colors = rainbow(10)) +
1689199443069:theme(text = element_text(size=16)) +
1689199443069:labs(x="diameter", y="number of samples", title = "a) K=20-means")
1689199444606:cplot_20
1689199579611:# Plotin'
1689199579612:cplot_5
1689199585887:cplot_5 <- model_5$centers %>%
1689199585889:ggplot(aes(diameter, e, color=e)) +
1689199585889:geom_point(size=5) +
1689199585890:scale_color_gradientn(colors = rainbow(10)) +
1689199585890:theme(text = element_text(size=16)) +
1689199585891:labs(x="diameter", y="number of samples", title = "a) K=5-means")
1689199587855:# Plotin'
1689199587857:cplot_5
1689199591501:cplot_10 <- model_10$centers %>%
1689199591502:ggplot(aes(diameter, e, color=e)) +
1689199591503:geom_point(size=5) +
1689199591503:scale_color_gradientn(colors = rainbow(10)) +
1689199591504:theme(text = element_text(size=16)) +
1689199591504:labs(x="diameter", y="number of samples", title = "a) K=10-means")
1689199593344:cplot_10
1689199595527:cplot_20
1689199747462:cplot_5 <- model_5$centers %>%
1689199747463:ggplot(aes(diameter, e, color=e)) +
1689199747463:geom_point(size=5) +
1689199747464:scale_color_gradientn(colors = rainbow(10)) +
1689199747464:theme(text = element_text(size=16)) +
1689199747464:labs(x="diameter", y="Eccentricity (shape)", title = "a) K=5-means")
1689199749267:cplot_10 <- model_10$centers %>%
1689199749268:ggplot(aes(diameter, e, color=e)) +
1689199749268:geom_point(size=5) +
1689199749269:scale_color_gradientn(colors = rainbow(10)) +
1689199749269:theme(text = element_text(size=16)) +
1689199749270:labs(x="diameter", y="Eccentricity (shape)", title = "a) K=10-means")
1689199750840:cplot_20 <- model_20$centers %>%
1689199750841:ggplot(aes(diameter, e, color=e)) +
1689199750841:geom_point(size=5) +
1689199750841:scale_color_gradientn(colors = rainbow(10)) +
1689199750842:theme(text = element_text(size=16)) +
1689199750842:labs(x="diameter", y="Eccentricity (shape)", title = "a) K=20-means")
1689199752562:# Plotin'
1689199752564:cplot_5
1689199754267:cplot_10
1689199755246:cplot_20
1689199774225:View(cp1)
1689199949400:spin()
1689199997847:spin("classification.r")
1689200151197:cplot_20
1689200155969:model_20 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 20)
1689200165722:model_20 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 20)
1689200173815:cplot_20 <- model_20$centers %>%
1689200173833:ggplot(aes(diameter, e, color=e)) +
1689200173834:geom_point(size=5) +
1689200173834:scale_color_gradientn(colors = rainbow(10)) +
1689200173834:theme(text = element_text(size=16)) +
1689200173834:labs(x="diameter", y="Eccentricity (shape)", title = "a) K=20-means")
1689200175768:cplot_20
1689200260992:---
1689200261009:title: "R Notebook"
1689200261010:output: html_notebook
1689200261010:---
1689200261011:This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.
1689200261012:Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.
1689200261013:```{r}
1689200261013:plot(cars)
1689200261064:```
1689200261064:Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
1689200261065:When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
1689200261066:The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
1689200269922:plot(cars)
1689200313088:---
1689200313088:title: "R Notebook"
1689200313089:output: html_notebook
1689200313094:---
1689200313095:install.packages("rmarkdown")
