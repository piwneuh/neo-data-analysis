1686160529137:library(sparklyr)
1686160529642:library(dplyr)
1686160529660:# Install
1686160529660:spark_install()
1686160565673:# Connect
1686160565673:sc <- sparklyr::spark_connect(master = "local")
1686160569637:datasetPath <- "/data/dataset.csv"
1686160569637:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686160581401:glimpse(df)
1686160582170:head(df)
1686160582736:spark_disconnect(sc)
1686161627258:# Display column names
1686161627259:colnames(data)
1686161630303:# Display summary statistics
1686161630304:summary(data)
1686161634525:# Explore data distribution
1686161634527:hist(data$column_name)
1686161643488:# Connect
1686161643489:sc <- sparklyr::spark_connect(master = "local")
1686161647219:datasetPath <- "/data/dataset.csv"
1686161649549:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686161783417:df <- spark_read_csv(sc, name = "my_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686161798877:# Display column names
1686161798879:colnames(df)
1686161802481:# Display summary statistics
1686161802482:summary(df)
1686161805179:# Explore data distribution
1686161805180:hist(df$column_name)
1686162217011:hist(df$diameter)
1686162323146:hist(diameter)
1686162501435:# Exploring data
1686162501436:colnames(df)
1686162503769:summary(df)
1686162507031:hist()
1686162530453:df$diameter
1686162534647:df$diameter
1686162673286:# Exploring data
1686162673287:colSums(df)
1686162698676:# Exploring data
1686162698677:colnames(df)
1686162702456:summary(df)
1686162905134:selected_df <- df %>%
1686162905135:select(id, full_name, H, diameter)
1686162925472:grouped_df <- df %>%
1686162925473:group_by(class) %>%
1686162925474:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686162942975:local_df <- collect(selected_df)
1686162960851:selected_df
1686162967083:grouped_df
1686162979728:local_df
1686163154975:# Classification
1686163154977:library(caret)
1686163172763:# Split the data into training and testing sets
1686163172763:set.seed(123)  # For reproducibility
1686163204158:train_indices <- createdfPartition(df$target_variable, p = 0.7, list = FALSE)
1686163208629:# Classification
1686163208630:library(caret)
1686163273513:# Classification
1686163273514:# Install and load required packages
1686163273514:install.packages("mlr")
1686163532224:library(mlr)
1686163532225:set.seed(123)
1686163532225:train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
1686163532244:training_set <- df[train_indices, ]
1686163532244:testing_set <- df[-train_indices, ]
1686163532245:train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
1686163532262:training_set <- df[train_indices, ]
1686163532262:testing_set <- df[-train_indices, ]
1686163532263:training_set
1686163532263:testing_set
1686163532263:trained_model1 <- train(model1, training_set)
1686163532264:trained_model2 <- train(model2, training_set)
1686163532264:trained_model3 <- train(model3, training_set)
1686163532264:trained_model4 <- train(model4, training_set)
1686163532265:trained_model5 <- train(model5, training_set)
1686163532265:# Define the classification models
1686163532265:model1 <- makeLearner("classif.logreg")
1686163532265:model2 <- makeLearner("classif.rpart")
1686163532266:model3 <- makeLearner("classif.randomForest")
1686163532266:model4 <- makeLearner("classif.svm")
1686163532266:model5 <- makeLearner("classif.naiveBayes")
1686163532266:# Train the models
1686163532267:trained_model1 <- train(model1, training_set)
1686163532267:trained_model2 <- train(model2, training_set)
1686163532268:trained_model3 <- train(model3, training_set)
1686163532268:trained_model4 <- train(model4, training_set)
1686163532268:trained_model5 <- train(model5, training_set)
1686163532268:# Make predictions on the testing set
1686163532269:predictions1 <- predict(trained_model1, newdata = testing_set)
1686163532269:predictions2 <- predict(trained_model2, newdata = testing_set)
1686163532269:predictions3 <- predict(trained_model3, newdata = testing_set)
1686163532270:predictions4 <- predict(trained_model4, newdata = testing_set)
1686163532270:predictions5 <- predict(trained_model5, newdata = testing_set)
1686163532270:# Evaluate the models
1686163532270:eval1 <- performance(predictions1, measures = list(acc))
1686163532271:eval2 <- performance(predictions2, measures = list(acc))
1686163532271:eval3 <- performance(predictions3, measures = list(acc))
1686163532271:eval4 <- performance(predictions4, measures = list(acc))
1686163532272:eval5 <- performance(predictions5, measures = list(acc))
1686163532272:performance <- data.frame(
1686163532272:Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
1686163532272:Accuracy = c(eval1$results$acc, eval2$results$acc, eval3$results$acc, eval4$results$acc, eval5$results$acc)
1686163532272:)
1686163532273:performance
1686163532273:# Classification
1686163532274:library(caret)
1686163532275:# Classification
1686163532275:library(caret)
1686163532275:library(caret)
1686163691656:install.packages("caret")
1686163873791:selected_df
1686163873999:selected_df
1686163913098:# Classification
1686163913100:library(caret)
1686163924066:# End the session
1686163924067:spark_disconnect(sc)
1686163999942:library(sparklyr)
1686163999943:library(dplyr)
1686163999944:# Install
1686163999944:spark_install()
1686163999962:# Connect
1686163999963:sc <- sparklyr::spark_connect(master = "local")
1686164003890:dfsetPath <- "/df/dfset.csv"
1686164003891:df <- spark_read_csv(sc, name = "my_df", path = dfsetPath, header = TRUE, infer_schema = TRUE)
1686164007037:# End the session
1686164007037:spark_disconnect(sc)
1686164053422:# End the session
1686164053423:spark_disconnect(sc)
1686164127467:library(sparklyr)
1686164127468:library(dplyr)
1686164127469:# Install
1686164127470:spark_install()
1686164127523:# Connect
1686164127524:sc <- sparklyr::spark_connect(master = "local")
1686164131272:datasetPath <- "/data/dataset.csv"
1686164131272:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164142071:# End the session
1686164142072:spark_disconnect(sc)
1686164143917:df
1686164267080:library(sparklyr)
1686164267080:library(dplyr)
1686164267081:# Install
1686164267082:spark_install()
1686164267125:# Connect
1686164267125:sc <- sparklyr::spark_connect(master = "local")
1686164269445:datasetPath <- "/data/dataset.csv"
1686164269445:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164270375:df
1686164271380:# End the session
1686164271380:spark_disconnect(sc)
1686164271381:spark_disconnect(sc)
1686164271383:library(sparklyr)
1686164271383:library(dplyr)
1686164271383:# Install
1686164271383:spark_install()
1686164271404:# Connect
1686164271404:sc <- sparklyr::spark_connect(master = "local")
1686164272838:datasetPath <- "/data/dataset.csv"
1686164272838:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164273846:# Display data
1686164273846:glimpse(df)
1686164274861:head(df)
1686164275876:# Exploring data
1686164275877:colnames(df)
1686164275879:summary(df)
1686164275881:# Select specific columns
1686164275881:selected_df <- df %>%
1686164275882:select(id, full_name, H, diameter)
1686164275884:selected_df
1686164276906:# Group by a column and compute aggregate functions
1686164276907:grouped_df <- df %>%
1686164276908:group_by(class) %>%
1686164276908:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686164276911:grouped_df
1686164277937:# Collect the resulting data to the local R environment
1686164277938:local_df <- collect(selected_df)
1686164278964:local_df
1686164278991:spark_disconnect(sc)
1686164278993:# Connect
1686164278994:sc <- sparklyr::spark_connect(master = "local")
1686164281394:# Connect
1686164281395:sc <- sparklyr::spark_connect(master = "local")
1686164283791:sc <- sparklyr::spark_connect(master = "local")
1686164288930:ddd
1686164293564:4 + 5
1686164304569:spark_disconnect(sc)
1686164310980:4-5
1686164335547:library(sparklyr)
1686164335549:library(dplyr)
1686164335549:# Install
1686164335550:spark_install()
1686164335571:# Connect
1686164335571:sc <- sparklyr::spark_connect(master = "local")
1686164338806:datasetPath <- "/data/dataset.csv"
1686164338806:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164349476:# Display data
1686164349477:glimpse(df)
1686164350196:head(df)
1686164350904:# Exploring data
1686164350904:colnames(df)
1686164350905:summary(df)
1686164350906:# Select specific columns
1686164350906:selected_df <- df %>%
1686164350907:select(id, full_name, H, diameter)
1686164350908:selected_df
1686164351211:# Group by a column and compute aggregate functions
1686164351211:grouped_df <- df %>%
1686164351212:group_by(class) %>%
1686164351212:summarise(mean_H = mean(H), max_diameter = max(diameter))
1686164351214:grouped_df
1686164351876:# Collect the resulting data to the local R environment
1686164351876:local_df <- collect(selected_df)
1686164355056:local_df
1686164355066:spark_disconnect(sc)
1686164848637:library(sparklyr)
1686164848639:library(dplyr)
1686164848639:# Install
1686164848639:spark_install()
1686164848658:# Connect
1686164848658:sc <- sparklyr::spark_connect(master = "local")
1686164851894:datasetPath <- "/data/dataset.csv"
1686164851894:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686164862896:df
1686164863678:# End the session
1686164863679:spark_disconnect(sc)
1686165332601:library(sparklyr)
1686165332602:library(dplyr)
1686165332603:# Install
1686165332603:spark_install()
1686165332624:# Connect
1686165332625:sc <- sparklyr::spark_connect(master = "local")
1686165335767:datasetPath <- "/data/dataset.csv"
1686165335767:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686165347792:# Sanity
1686165347792:glimpse(df)
1686165348561:# Comparison of classification models with different parameter values
1686165348561:model_params <- list(
1686165348562:list(param1 = value1),
1686165348562:list(param1 = value2),
1686165348562:...
1686165348562:)
1686165348563:# Define a function to create and evaluate classification models
1686165348563:evaluate_classification_model <- function(params) {
1686165348564:# Create the classification model with the specified parameter values
1686165348564:model <- spark.ml.function_to_create_model(params)
1686165348564:# Split the data into training and testing sets
1686165348564:splits <- randomSplit(df, c(0.7, 0.3), seed = 123)
1686165348564:training_set <- splits[[1]]
1686165348565:testing_set <- splits[[2]]
1686165348565:# Fit the model on the training set
1686165348565:trained_model <- ml.fit(model, training_set)
1686165348565:# Make predictions on the testing set
1686165348566:predictions <- ml.transform(trained_model, testing_set)
1686165348566:# Calculate performance metrics (e.g., accuracy, precision, recall, F1-score)
1686165348566:performance <- ml.function_to_calculate_performance(predictions)
1686165348567:# Return the performance metrics
1686165348567:return(performance)
1686165348567:}
1686165348568:# Perform comparison of classification models with different parameter values
1686165348568:for (params in model_params) {
1686165348568:performance <- evaluate_classification_model(params)
1686165348568:print(performance)
1686165348568:}
1686165348571:# Comparison of classification models with different types
1686165348571:model_types <- list("model_type1", "model_type2", ...)
1686165348571:# Define a function to create and evaluate classification models
1686165348572:evaluate_classification_model_type <- function(model_type) {
1686165348572:# Create the classification model of the specified type
1686165348572:model <- spark.ml.function_to_create_model_type(model_type)
1686165348572:# Split the data into training and testing sets
1686165348572:splits <- randomSplit(df, c(0.7, 0.3), seed = 123)
1686165348573:training_set <- splits[[1]]
1686165348573:testing_set <- splits[[2]]
1686165348573:# Fit the model on the training set
1686165348573:trained_model <- ml.fit(model, training_set)
1686165348574:# Make predictions on the testing set
1686165348574:predictions <- ml.transform(trained_model, testing_set)
1686165348574:# Calculate performance metrics (e.g., accuracy, precision, recall, F1-score)
1686165348575:performance <- ml.function_to_calculate_performance(predictions)
1686165348575:# Return the performance metrics
1686165348576:return(performance)
1686165348576:}
1686165348576:# Perform comparison of classification models with different types
1686165348577:for (model_type in model_types) {
1686165348577:performance <- evaluate_classification_model_type(model_type)
1686165348577:print(performance)
1686165348577:}
1686165348580:# Clustering analysis with different parameter values
1686165348580:clustering_params <- list(
1686165348580:list(param1 = value1),
1686165348580:list(param1 = value2),
1686165348580:...
1686165348580:)
1686165348581:# Define a function to perform clustering analysis
1686165348581:perform_clustering <- function(params) {
1686165348581:# Perform clustering with the specified parameter values
1686165348581:clusters <- spark.ml.function_to_perform_clustering(params)
1686165348582:# Analyze and interpret the obtained clusters
1686165348582:analysis <- spark.ml.function_to_analyze_clusters(clusters)
1686165348582:# Return the analysis results
1686165348583:return(analysis)
1686165348583:}
1686165348583:# Perform clustering analysis with different parameter values
1686165348583:for (params in clustering_params) {
1686165348584:analysis <- perform_clustering(params)
1686165348584:print(analysis)
1686165348584:}
1686165348586:# Generate the analytical report
1686165348586:report <- create_report(df, model_params, model_types, clustering_params)
1686165348586:save_report(report, "report.pdf")
1686165348587:# End the session
1686165348587:spark_disconnect(sc)
1686165639283:library(sparklyr)
1686165639285:library(dplyr)
1686165639286:# Install
1686165639286:#spark_install()
1686165639287:# Connect
1686165639287:sc <- sparklyr::spark_connect(master = "local")
1686165642450:datasetPath <- "/data/dataset.csv"
1686165642450:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1686165653528:# Sanity
1686165653528:glimpse(df)
1686165654246:# End the session
1686165654246:spark_disconnect(sc)
1689160715882:library(sparklyr)
1689160716890:library(dplyr)
1689160716926:# Install
1689160716927:spark_install()
1689160717480:# Connect
1689160717480:sc <- sparklyr::spark_connect(master = "local")
1689160723003:datasetPath <- "/data/dataset.csv"
1689160723004:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689160735584:# Display data
1689160735584:glimpse(df)
1689160736356:head(df)
1689160736985:# Exploring data
1689160736985:colnames(df)
1689160736986:summary(df)
1689160736987:# Select specific columns
1689160736987:selected_df <- df %>%
1689160736987:select(id, full_name, H, diameter)
1689160736990:selected_df
1689160737286:# Group by a column and compute aggregate functions
1689160737286:grouped_df <- df %>%
1689160737286:group_by(class) %>%
1689160737287:summarise(mean_H = mean(H), max_diameter = max(diameter))
1689160737290:grouped_df
1689160738040:# Collect the resulting data to the local R environment
1689160738040:local_df <- collect(selected_df)
1689160741783:local_df
1689160741793:spark_disconnect(sc)
1689160916482:# Connect
1689160916484:sc <- sparklyr::spark_connect(master = "local")
1689160922028:datasetPath <- "/data/dataset.csv"
1689160922028:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689160934091:# Sanity
1689160934091:glimpse(df)
1689161687334:df_cleaned <- df %>%
1689161687335:filter(!is.na(column_name))
1689161701932:glimpse(df_cleaned)
1689161936868:install.packages("tidyverse")
1689162016901:install.packages("tidyverse", repos = "https://cran.rstudio.com/")
1689162396737:library(tidyr)
1689162408758:install.packages("gridExtra")
1689162434182:install.packages("gridExtra", repos = "https://cran.rstudio.com/")
1689162482052:install.packages("kableExtra", repos = "https://cran.rsduio.com/")
1689162507508:install.packages("kableExtra", repos = "https://cran.rsdtuio.com/")
1689162558565:library(sparklyr)
1689162559088:library(dplyr)
1689162559103:library(tidyr)
1689162559103:library(ggplot2)
1689162559460:library(magrittr)
1689162559472:library(knitr)
1689162559501:library(kableExtra)
1689162559503:library(cowplot)
1689162601574:# Install
1689162601592:spark_install()
1689162614129:# Sanity
1689162614147:glimpse(df)
1689162625814:# End the session
1689162625815:spark_disconnect(sc)
1689162631555:# Connect
1689162631557:sc <- sparklyr::spark_connect(master = "local")
1689162645844:datasetPath <- "/data/dataset.csv"
1689162648445:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689162664391:# Sanity
1689162664392:glimpse(df)
1689162691037:# Columns
1689162691037:colnames(df)
1689162821513:df <- df %>%
1689162821515:mutate(abs_mag_category = case_when(
1689162821516:H < 15 ~ "Very Bright",
1689162821516:H >= 15 & H < 20 ~ "Moderately Bright",
1689162821517:TRUE ~ "Dim"
1689162821517:))
1689162840213:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689162840231:df <- df %>%
1689162840231:mutate(orbit_class_category = case_when(
1689162840232:startsWith(orbit_id, "M") ~ "Main Belt",
1689162840232:startsWith(orbit_id, "O") ~ "Outer Solar System",
1689162840233:startsWith(orbit_id, "I") ~ "Inner Solar System",
1689162840234:TRUE ~ "Unknown"
1689162840234:))
1689162844484:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689162844485:df <- df %>%
1689162844486:mutate(size_category = case_when(
1689162844486:diameter < 1 ~ "Small",
1689162844487:diameter >= 1 & diameter <= 10 ~ "Medium",
1689162844488:diameter > 10 ~ "Large",
1689162844488:TRUE ~ "Unknown"
1689162844489:))
1689162867839:# Sanity columns
1689162867841:colnames(df)
1689167979628:library(sparklyr)
1689167988155:library(dplyr)
1689167991487:library(tidyr)
1689167995119:library(ggplot2)
1689167997098:library(magrittr)
1689167998673:library(knitr)
1689168006647:library(sparklyr)
1689168016855:# Install
1689168016857:spark_install()
1689168056174:install.packages("sparklyr")
1689168089419:install.packages("sparklyr", repos="https://cran.rstudio.com")
1689168145466:library(sparklyr)
1689168260753:# Install
1689168260755:spark_install()
1689168269279:# Install
1689168269297:spark_install()
1689168284860:sc <- sparklyr::spark_connect(master = "local")
1689168295985:datasetPath <- "/data/dataset.csv"
1689168300126:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689168316633:# Columns
1689168316634:colnames(df)
1689168465443:# Filter data
1689168465444:df <- df %>%
1689168465445:filter(!is.na(diameter_sigma))
1689168471032:# Exclude the "prefix" column
1689168471033:df <- select(df, -prefix)
1689168490258:# Columns sanity
1689168490260:colnames(df)
1689168500835:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689168500837:df <- df %>%
1689168500837:mutate(abs_mag_category = case_when(
1689168500838:H < 15 ~ "Very Bright",
1689168500838:H >= 15 & H < 20 ~ "Moderately Bright",
1689168500839:TRUE ~ "Dim"
1689168500839:))
1689168502344:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689168502345:df <- df %>%
1689168502346:mutate(orbit_class_category = case_when(
1689168502346:startsWith(orbit_id, "M") ~ "Main Belt",
1689168502347:startsWith(orbit_id, "O") ~ "Outer Solar System",
1689168502347:startsWith(orbit_id, "I") ~ "Inner Solar System",
1689168502348:TRUE ~ "Unknown"
1689168502348:))
1689168513367:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689168513368:df <- df %>%
1689168513369:mutate(size_category = case_when(
1689168513369:diameter < 1 ~ "Small",
1689168513370:diameter >= 1 & diameter <= 10 ~ "Medium",
1689168513370:diameter > 10 ~ "Large",
1689168513371:TRUE ~ "Unknown"
1689168513371:))
1689168547390:# Create a new column that categorizes asteroids into different orbit class categories based on their orbit class
1689168547392:df <- df %>%
1689168547392:mutate(orbit_class_category = case_when(
1689168547393:"M" ~ "Main Belt",
1689168547393:"O" ~ "Outer Solar System",
1689168547394:"I" ~ "Inner Solar System",
1689168547394:TRUE ~ "Unknown"
1689168547395:))
1689168634038:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689168634040:df <- df %>%
1689168634040:mutate(abs_mag_category = case_when(
1689168634040:H < 15 ~ "Very Bright",
1689168634041:H >= 15 & H < 20 ~ "Moderately Bright",
1689168634041:TRUE ~ "Dim"
1689168634041:))
1689168636299:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689168636301:df <- df %>%
1689168636301:mutate(size_category = case_when(
1689168636302:diameter < 1 ~ "Small",
1689168636302:diameter >= 1 & diameter <= 10 ~ "Medium",
1689168636302:diameter > 10 ~ "Large",
1689168636303:TRUE ~ "Unknown"
1689168636303:))
1689168639370:# Sanity columns
1689168639371:colnames(df)
1689168907296:# Classification modeling
1689168907298:# Regression
1689168907298:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H", "diameter"))
1689168930285:# Classification modeling
1689168930287:# Regression
1689168930288:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689168974091:# Data split
1689168974093:df_split <- sdf_partition(df, training = 0.7, test = 0.3)
1689168989560:# Data split
1689168989578:df_split <- df_random_split(df, training = 0.7, test = 0.3)
1689169003466:# Data split
1689169003467:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169010706:df_split
1689169400789:# Data split
1689169400790:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169402257:# Data split
1689169402259:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169463589:result <- ml_evaluate(m1, data_split$test)
1689169481165:m1 <- ml_fit(model1, data_split$training)
1689169490379:m1 <- ml_fit(model1, df_split$training)
1689169600666:df_split <- sdf_with_column(df_split, "H", cast(col("H"), "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689169691436:# Data split
1689169691438:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689169696149:# Check the split out
1689169696151:df_split
1689170214265:# Type conversion for H
1689170214266:df <- df %>%
1689170214267:mutate(H = cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170230534:# Regression
1689170230536:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689170356980:# Type conversion for H
1689170356982:df <- df %>%
1689170356982:mutate(H = cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170419817:# Type conversion for H
1689170419819:df <- df %>%
1689170419820:select(-H) %>%
1689170419820:mutate(H = cast(col("H"), "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>")) %>%
1689170419821:select(H, everything())
1689170468663:# Type conversion for H
1689170468664:df <- df %>%
1689170468664:mutate(H = alias(H, cast(H, "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>")))
1689170526184:# Type conversion for H
1689170526186:df <- df %>%
1689170526186:mutate(H = `H` %>% cast("struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170565663:# Type conversion for H
1689170565664:df <- df %>%
1689170565665:mutate("H" =  "H" %>% cast("struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"))
1689170579598:# Data split
1689170579600:df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
1689170589489:# Check the split out
1689170589491:df_split
1689170601075:# Regression
1689170601076:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("H"))
1689170614326:View(df_split)
1689170617941:View(model1)
1689170646156:# Traing the model
1689170646157:m1 <- ml_fit(model1, df_split$training)
1689170674716:# Regression
1689170674717:model1 <- ml_logistic_regression(sc, label_col = "H", features_col = c("albedo"))
1689170677475:# Traing the model
1689170677477:m1 <- ml_fit(model1, df_split$training)
1689170869358:# Regression
1689170869360:model1 <- ml_logistic_regression(sc, label_col = "epoch", features_col = c("epoch_mjd"))
1689170872214:# Traing the model
1689170872216:m1 <- ml_fit(model1, df_split$training)
1689171156949:# Show data
1689171156950:kable(head(df, n=10L),
1689171156951:col.names = c("diameter",
1689171156951:"H",
1689171156952:"albedo",
1689171156952:"abs_mag_category",
1689171156953:"size_category"),
1689171156953:label = "Table view of filtered data",
1689171156954:format = "html",
1689171156954:align = "ccccc"
1689171156955:)
1689171211241:# Show data
1689171211243:kable(head(df, n=10L),
1689171211243:col.names = c("diameter",
1689171211244:"H",
1689171211244:"albedo",
1689171211245:"abs_mag_category",
1689171211245:"size_category"),
1689171211245:label = "Table view of filtered data",
1689171211246:format = "html",
1689171211246:align = "ccccc"
1689171211247:)
1689171312754:# Show data
1689171312756:kable(head(df, n = 10L),
1689171312756:col.names = c("diameter",
1689171312757:"H",
1689171312757:"albedo",
1689171312758:"abs_mag_category",
1689171312758:"size_category"),
1689171312758:caption = "Table view of filtered data",
1689171312759:format = "html",
1689171312759:align = "c"
1689171312760:)
1689171361005:# Show data
1689171361007:kable(head(df, n = 10L),
1689171361007:col.names = colnames(df),
1689171361008:caption = "Table view of filtered data",
1689171361008:format = "html",
1689171361009:align = "c"
1689171361009:)
1689171634004:# Regression
1689171634006:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689171636962:# Traing the model
1689171636964:m1 <- ml_fit(model1, df_split$training)
1689171715142:# Check the split out
1689171715143:df_split
1689171720791:View(model1)
1689171727458:# Regression
1689171727460:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689171737974:# Regression
1689171737976:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter", "H"))
1689171748358:rlang::last_trace(
1689171750022:)
1689171995050:# Show data
1689171995052:kable(head(df, n = 10L),
1689171995052:col.names = colnames(df),
1689171995053:caption = "Table view of filtered data",
1689171995053:format = "html",
1689171995054:align = "c"
1689171995054:) %>%
1689171995055:kableExtra::kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16)
1689172960644:spark_disconnect(df_spark)
1689172981279:# Show data
1689172981280:kable(head(df, n = 10L),
1689172981281:col.names = colnames(df),
1689172981281:caption = "Table view of filtered data",
1689172981282:format = "html",
1689172981282:align = "c"
1689172981283:)
1689173072608:# Regression
1689173072610:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter", "H"))
1689173079819:# Regression
1689173079820:model1 <- ml_logistic_regression(sc, label_col = "albedo", features_col = c("diameter"))
1689173082396:# Traing the model
1689173082397:m1 <- ml_fit(model1, df_split$training)
1689173087672:# Regression
1689173087674:model1 <- ml_logistic_regression(df, label_col = "albedo", features_col = c("diameter"))
1689173543074:slelect(albedo, H, diameter, full_name)
1689173547741:select(albedo, H, diameter, full_name)
1689173586870:df %>% select(albedo, H, diameter, full_name)
1689173601631:df
1689173655048:# Select data for further testing
1689173655050:dataset <- df %>% select(albedo, H, diameter, full_name)
1689173667661:# Data split
1689173667679:df_split <- sdf_random_split(dataset, training = 0.7, test = 0.3)
1689173670843:# Check the split out
1689173670845:df_split
1689173714238:# Filter data
1689173714240:df <- df %>%
1689173714240:filter(!is.na(diameter_sigma)||
1689173714241:!is.na(albedo))
1689173728525:# Filter data
1689173728527:df <- df %>%
1689173728527:filter(!is.na(diameter_sigma)||
1689173728528:!is.na(albedo))
1689173734497:# Exclude the "prefix" column
1689173734499:df <- select(df, -prefix)
1689173756604:# Select data for further testing
1689173756605:dataset <- df %>% select(albedo, H, diameter, full_name)
1689173760114:# Data split
1689173760115:df_split <- sdf_random_split(dataset, training = 0.7, test = 0.3)
1689173762139:# Check the split out
1689173762141:df_split
1689173807029:# Formula
1689173807031:formula <- (1329 * 10 ^ (-H/5))/diameter)^2
1689173818740:# Formula
1689173818741:formula <- ((1329 * 10 ^ (-H/5))/diameter)^2)
1689173827862:# Formula
1689173827862:formula <- ((1329 * 10 ^(-H/5))/diameter)^2)
1689174346812:# End the session
1689174346813:spark_disconnect(sc)
1689174354058:View(dataset)
1689174414011:# Install
1689174414012:spark_install()
1689174420816:# Connect
1689174420817:sc <- sparklyr::spark_connect(master = "local")
1689174473986:# Connect
1689174473988:sc <- sparklyr::spark_connect(master = "local")
1689174484423:datasetPath <- "/data/dataset.csv"
1689174486582:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689174653775:# Filter data
1689174653776:df <- df %>%
1689174653777:filter(!is.na(diameter_sigma) ||
1689174653777:!is.na(albedo) ||
1689174653777:!is.na(H) ||
1689174653777:!is.na(diameter)||
1689174653778:!is.na(epoch) ||
1689174653778:!is.na(full_name)
1689174653778:)
1689174659318:View(df)
1689174670485:# Columns sanity
1689174670486:colnames(df)
1689174695166:df
1689174839488:# Create a new column with H divided by 5
1689174839490:df <- df %>%
1689174839491:mutate(new_column = withColumn("new_column", df$H / 5))
1689174855724:# Create a new column with H divided by 5
1689174855726:df <- df %>%
1689174855726:mutate(new_column = withColumn("H/5", df$H / 5))
1689174925127:# Create a new column with H divided by 5
1689174925129:df <- df %>%
1689174925129:mutate(H/5 = H / 5)
1689174935521:# Create a new column with H divided by 5
1689174935522:df <- df %>%
1689174935523:mutate(H/5 ~ H / 5)
1689174942529:# Create a new column with H divided by 5
1689174942529:df <- df %>%
1689174942530:mutate(H5 ~ H / 5)
1689174953465:# Create a new column with H divided by 5
1689174953467:df <- df %>%
1689174953467:mutate(new_column ~ H / 5)
1689174975088:# Create a new column with H divided by 5
1689174975090:df <- df %>%
1689174975090:mutate(new_column = H / 5)
1689174989011:colnames(df)
1689175055287:rename(new_column = H5)
1689175066043:rename(new_column = "h_5")
1689175071971:rename(new_column = h_5)
1689175089588:rename(new_column = h)
1689175116470:# Create a new column with H divided by 5 (latter used in the formula)
1689175116472:df <- df %>%
1689175116472:mutate(new_column = H / 5) %>%
1689175116473:rename(new_column = h)
1689175133919:# Create a new column with H divided by 5 (latter used in the formula)
1689175133920:df <- df %>%
1689175133921:mutate(new_column = H / 5) %>%
1689175133921:rename(h = new_column)
1689175142071:colnames(df)
1689175150839:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175150841:df <- df %>%
1689175150842:mutate(abs_mag_category = case_when(
1689175150842:H < 15 ~ "Very Bright",
1689175150843:H >= 15 & H < 20 ~ "Moderately Bright",
1689175150843:TRUE ~ "Dim"
1689175150844:))
1689175186165:# Create a new column with H divided by 5 (latter used in the formula)
1689175186183:df <- df %>%
1689175186184:mutate(new_column = H / 5) %>%
1689175186184:rename(H5 = new_column)
1689175193123:# Create a new column with H divided by 5 (latter used in the formula)
1689175193124:df <- df %>%
1689175193125:mutate(new_column = H / 5) %>%
1689175193125:rename(H5 = h)
1689175208710:# Create a new column with H divided by 5 (latter used in the formula)
1689175208712:df <- df %>%
1689175208712:mutate(new_column = H / 5) %>%
1689175208713:rename(h = new_column)
1689175215501:colnames(h)
1689175221598:colnames(df)
1689175229917:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175229918:df <- df %>%
1689175229919:mutate(abs_mag_category = case_when(
1689175229920:H < 15 ~ "Very Bright",
1689175229920:H >= 15 & H < 20 ~ "Moderately Bright",
1689175229921:TRUE ~ "Dim"
1689175229921:))
1689175249142:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175249144:df <- df %>%
1689175249145:mutate(abs_mag_category = case_when(
1689175249145:df$H < 15 ~ "Very Bright",
1689175249146:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175249146:TRUE ~ "Dim"
1689175249147:))
1689175259887:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175259888:df <- df %>%
1689175259889:mutate(abs_mag_category = case_when(
1689175259889:H < 15 ~ "Very Bright",
1689175259890:H >= 15 & H < 20 ~ "Moderately Bright",
1689175259890:TRUE ~ "Dim"
1689175259891:))
1689175320494:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175320495:df <- df %>%
1689175320496:mutate(size_category = case_when(
1689175320496:diameter < 1 ~ "Small",
1689175320497:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175320498:diameter > 10 ~ "Large",
1689175320498:TRUE ~ "Unknown"
1689175320499:))
1689175331327:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175331328:df <- df %>%
1689175331329:mutate(abs_mag_category = case_when(
1689175331329:h < 15 ~ "Very Bright",
1689175331330:h >= 15 & h < 20 ~ "Moderately Bright",
1689175331330:TRUE ~ "Dim"
1689175331331:))
1689175367379:df <- df %>% rename(H5 = h)
1689175376946:colnames(df
1689175376948:)
1689175393547:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175393548:df <- df %>%
1689175393548:mutate(abs_mag_category = case_when(
1689175393549:H < 15 ~ "Very Bright",
1689175393549:H >= 15 & H < 20 ~ "Moderately Bright",
1689175393550:TRUE ~ "Dim"
1689175393550:))
1689175419877:# Create a new column with H divided by 5 (latter used in the formula)
1689175419879:df <- df %>%
1689175419880:rename(h = H5)
1689175452951:# End the session
1689175452953:spark_disconnect(sc)
1689175465582:# Connect
1689175465584:sc <- sparklyr::spark_connect(master = "local")
1689175491032:datasetPath <- "/data/dataset.csv"
1689175491773:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175519948:# Filter data
1689175519950:df <- df %>%
1689175519951:filter(!is.na(diameter_sigma) ||
1689175519951:!is.na(albedo) ||
1689175519952:!is.na(H) ||
1689175519952:!is.na(diameter)||
1689175519953:!is.na(epoch) ||
1689175519953:!is.na(full_name)
1689175519954:)
1689175524359:# Create a new column with H divided by 5 (latter used in the formula)
1689175524360:df <- df %>%
1689175524361:mutate(new_column = H / 5) %>%
1689175524361:rename(h = new_column)
1689175537270:colnames(df)
1689175550156:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175550158:df <- df %>%
1689175550158:mutate(abs_mag_category = case_when(
1689175550159:H < 15 ~ "Very Bright",
1689175550160:H >= 15 & H < 20 ~ "Moderately Bright",
1689175550160:TRUE ~ "Dim"
1689175550161:))
1689175575570:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175575572:df <- df %>%
1689175575572:mutate(abs_mag_category = case_when(
1689175575573:df$H < 15 ~ "Very Bright",
1689175575573:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175575574:TRUE ~ "Dim"
1689175575575:))
1689175597377:df
1689175605041:df
1689175637242:# End the session
1689175637243:spark_disconnect(sc)
1689175650731:# Connect
1689175650749:sc <- sparklyr::spark_connect(master = "local")
1689175660200:datasetPath <- "/data/dataset.csv"
1689175660203:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175669188:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689175680389:# Filter data
1689175680391:df <- df %>%
1689175680391:filter(!is.na(diameter_sigma) ||
1689175680392:!is.na(albedo) ||
1689175680392:!is.na(H) ||
1689175680393:!is.na(diameter)||
1689175680393:!is.na(epoch) ||
1689175680394:!is.na(full_name)
1689175680394:)
1689175684389:df
1689175744492:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175744494:df <- df %>%
1689175744494:mutate(abs_mag_category = case_when(
1689175744495:df$H < 15 ~ "Very Bright",
1689175744495:df$H >= 15 & df$H < 20 ~ "Moderately Bright",
1689175744496:TRUE ~ "Dim"
1689175744496:))
1689175758566:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175758568:df <- df %>%
1689175758568:mutate(abs_mag_category = case_when(
1689175758569:H < 15 ~ "Very Bright",
1689175758570:H >= 15 & H < 20 ~ "Moderately Bright",
1689175758570:TRUE ~ "Dim"
1689175758571:))
1689175761525:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175761526:df <- df %>%
1689175761527:mutate(size_category = case_when(
1689175761527:diameter < 1 ~ "Small",
1689175761528:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175761528:diameter > 10 ~ "Large",
1689175761529:TRUE ~ "Unknown"
1689175761529:))
1689175763582:# Sanity columns
1689175763583:colnames(df)
1689175879010:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689175879012:df <- df %>%
1689175879013:mutate(brightness = case_when(
1689175879013:H < 15 ~ "Very Bright",
1689175879014:H >= 15 & H < 20 ~ "Moderately Bright",
1689175879014:TRUE ~ "Dim"
1689175879015:))
1689175894549:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689175894550:df <- df %>%
1689175894551:mutate(size_category = case_when(
1689175894551:diameter < 1 ~ "Small",
1689175894552:diameter >= 1 & diameter <= 10 ~ "Medium",
1689175894552:diameter > 10 ~ "Large",
1689175894553:TRUE ~ "Unknown"
1689175894553:))
1689175912355:# Select most relevant data for further analising
1689175912356:df <- df %>% select(albedo, H, diameter, diameter_sigma, epoch, full_name, brightness, size_category)
1689175918599:# Show data
1689175918600:kable(head(df, n = 10L),
1689175918601:col.names = colnames(df),
1689175918602:caption = "Table view of filtered data",
1689175918602:format = "html",
1689175918603:align = "c"
1689175918603:)
1689176018963:# Select most relevant data for further analising
1689176018965:df <- df %>% select(name, albedo, H, diameter, diameter_sigma, epoch, brightness, size_category)
1689176023989:df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
1689176031327:# Filter data
1689176031327:df <- df %>%
1689176031328:filter(!is.na(diameter_sigma) ||
1689176031328:!is.na(albedo) ||
1689176031329:!is.na(H) ||
1689176031329:!is.na(diameter)||
1689176031330:!is.na(epoch) ||
1689176031330:!is.na(name)
1689176031331:)
1689176034039:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689176034042:df <- df %>%
1689176034043:mutate(brightness = case_when(
1689176034044:H < 15 ~ "Very Bright",
1689176034044:H >= 15 & H < 20 ~ "Moderately Bright",
1689176034045:TRUE ~ "Dim"
1689176034045:))
1689176036548:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689176036549:df <- df %>%
1689176036550:mutate(size_category = case_when(
1689176036550:diameter < 1 ~ "Small",
1689176036551:diameter >= 1 & diameter <= 10 ~ "Medium",
1689176036551:diameter > 10 ~ "Large",
1689176036552:TRUE ~ "Unknown"
1689176036552:))
1689176039348:# Sanity columns
1689176039350:colnames(df)
1689176088975:# H devided by 5 (needed for formula)
1689176088976:df <- df %>%
1689176088977:mutate(new_column = H / 5)
1689176090750:# Sanity columns
1689176090752:colnames(df)
1689176094453:# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
1689176094455:df <- df %>%
1689176094455:mutate(size_category = case_when(
1689176094456:diameter < 1 ~ "Small",
1689176094456:diameter >= 1 & diameter <= 10 ~ "Medium",
1689176094457:diameter > 10 ~ "Large",
1689176094457:TRUE ~ "Unknown"
1689176094458:))
1689176096669:# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
1689176096669:df <- df %>%
1689176096670:mutate(brightness = case_when(
1689176096670:H < 15 ~ "Very Bright",
1689176096671:H >= 15 & H < 20 ~ "Moderately Bright",
1689176096672:TRUE ~ "Dim"
1689176096672:))
1689176108932:# H devided by 5 (needed for formula)
1689176108932:df <- df %>%
1689176108933:mutate(H_devided = H / 5)
1689176111495:# Sanity columns
1689176111497:colnames(df)
1689176123399:# Select most relevant data for further analising
1689176123400:df <- df %>% select(name, albedo, H, H_devided, diameter, diameter_sigma, epoch, brightness, size_category)
1689176125899:# Show selected data
1689176125901:kable(head(df, n = 10L),
1689176125902:col.names = colnames(df),
1689176125902:caption = "Table view of filtered data",
1689176125903:format = "html",
1689176125903:align = "c"
1689176125904:)
