# Uvod

Predmetni projekat iz predmeta Racunarstvo visokih performansi u informacionom inzenjeringu

Filip Pinjuh, e254/2022

## Resursi korisceni u projektu

Asteroid dataset: https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset

Okruzenje: https://github.com/piwneuh/SparkR

Repozitorijum: https://github.com/piwneuh/neo-data-analysis

---

## Setting up the environment

```
# Install R packages
install.packages("tidyverse", repos = "https://cran.rstudio.com/")
install.packages("sparklyr", repos="https://cran.rstudio.com")
```

```
# Load libraries
library(sparklyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(magrittr)
library(knitr)
```

```
# Install Spark
spark_install()
sc <- sparklyr::spark_connect(master = "local")
```

```
# Load dataset
datasetPath <- "/data/dataset.csv"
df <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)
```

## Filtering and categorizing data

```
# Create a new column that categorizes asteroids into different brightness categories based on their absolute magnitude (H)
df <- df %>%
  mutate(brightness = case_when(
    albedo < 0.25 ~ "Can't be seen",
    albedo >= 0.25 ~ "Moderately Bright",
    TRUE ~ "Can't be seen"
  ))

# Create a new column 
df <- df %>%
  mutate(can_be_seen = case_when(
    albedo < 0.25 ~ 0,
    albedo >= 0.25 ~ 1,
    TRUE ~ 0
  ))

df <- df %>% select(albedo, H, H_divd, diameter, diameter_sigma, epoch, can_be_seen, brightness)

# Just in case
df <- na.omit(df)
```

// TODO: DATA TABLE

## Preparing data for classification
```
df_split <- sdf_random_split(df, training = 0.7, test = 0.3)
```

## Logistic regression
```
# Formula
formula <- can_be_seen ~ H + H_divd + diameter + albedo

# Logistic regression with different max iterations
max_iterations <- 5
result_data <- data.frame(
  i = 1:max_iterations, 
  wp = numeric(max_iterations), 
  wr = numeric(max_iterations), 
  a = numeric(max_iterations),
  f1 = numeric(max_iterations)
)

for (i in 1:max_iterations) {
  # Traing the model
  model <- ml_logistic_regression(df_split$training, formula, max_iter = i, family = "binomial")
  
  # Evaluate the model
  result <- ml_evaluate(model, df_split$test)
  
  # Save results
  result_data$wp[i] <- result$weighted_precision()
  result_data$wr[i] <- result$weighted_recall()
  result_data$a[i] <- result$accuracy()
  result_data$f1[i] <- result$weighted_f_measure()
}
```

## Ploting how the number of iterations affects performance of the model
//TODO: PLOTS


## Checking other models: Naive Bayes, Linear SVC, Decision Tree Classification
```
# Reaffirm formula
formula <- can_be_seen ~ H + H_divd + diameter + albedo

# Bayes classification
bayes_model <- df_split$training %>% ml_naive_bayes(formula)

# Linear SVC classification
linear_svc_model <- df_split$training %>%
  ml_linear_svc(formula)

# Decision 3 classification
decision_tree_classifier <- df_split$training %>%
  ml_decision_tree_classifier(formula)
```

## Evaluating the models
```
# Testing Accuracy Score for different methods using test dataset
bayes_accuracy <- ml_evaluate(bayes_model, df_split$test)$Accuracy
svc_accuracy <- ml_evaluate(linear_svc_model, df_split$test)$Accuracy
d3_accuracy <- ml_evaluate(decision_tree_classifier, df_split$test)$Accuracy

# Testing Accuracy Score for different methods using 4-cross validation
kk4c <- function(dataset, model, formula){
  dataset <- dataset %>%
    sdf_random_split(seed=1,
                     s1=0.25,
                     s2=0.25,
                     s3=0.25,
                     s4=0.25)
  training <- list(
    s1 = sdf_bind_rows(dataset$s2, dataset$s3, dataset$s4),
    s2 = sdf_bind_rows(dataset$s1, dataset$s3, dataset$s4),
    s3 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s4),
    s4 = sdf_bind_rows(dataset$s1, dataset$s2, dataset$s3)
  )
  
  trained = list(s1=model(training$s1, formula),
                 s2=model(training$s2, formula),
                 s3=model(training$s3, formula),
                 s4=model(training$s4, formula)
  )
  
  accuracy <- (ml_evaluate(trained$s1, dataset$s1)$Accuracy +
                    ml_evaluate(trained$s2, dataset$s2)$Accuracy +
                    ml_evaluate(trained$s3, dataset$s3)$Accuracy +
                    ml_evaluate(trained$s4, dataset$s4)$Accuracy
  ) / 4
}

bayes_k4_accuracy <- k4c(df, ml_naive_bayes, formula)
svc_k4_accuracy <- k4c(df, ml_linear_svc, formula)
d3_k4_accuracy <- k4c(df, ml_decision_tree_classifier, formula)
```
//TODO: TABLE

## Clusterization: getting the data ready
```
# Clusterizatio dataset
df_clusterization <- spark_read_csv(sc, name = "neo_data", path = datasetPath, header = TRUE, infer_schema = TRUE)

# Filter clusterization data
df_clusterization <- df_clusterization %>%
  filter(!(is.na(diameter) || is.na(e)))

# Create a new column that categorizes asteroids into different size categories based on their diameter (km)
df_clusterization <- df_clusterization %>%
  mutate(size_category = case_when(
    diameter < 1 ~ "Small",
    diameter >= 1 & diameter < 5 ~ "Medium",
    diameter >= 5 & diameter < 10 ~ "Large",
    diameter >= 10 ~ "X-Large",
    TRUE ~ "Unknown"
  ))
```

## Clusterization: K-Means
```
# Select columns relevant for clusterizations
df_clusterization <- df_clusterization %>% select(diameter, e, size_category)

# New Formula
cluster_formula <- size_category ~ diameter + e

# K-means (5, 10, 20)
model_5 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 5)
model_10 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 10)
model_10 <- ml_kmeans(df_clusterization, cluster_formula, seed = 1, k = 20)
```

//TODO: PLOTS

## Finally, disconect from spark
```
# End the session
spark_disconnect(sc)
```